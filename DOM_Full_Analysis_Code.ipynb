{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "<ipython-input-35-39a247cfb2a9>:812: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_left['perspective-order']='left'\n",
      "<ipython-input-35-39a247cfb2a9>:824: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_left[ql5c]= np.where(df_left[ql5c]==5, 0, 1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1736: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n",
      "<ipython-input-35-39a247cfb2a9>:834: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_left[ql4c]= np.where(df_left[ql4c]==4, 0, 1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1736: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##### import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "#upload raw data from inline questionnaire\n",
    "raw_onlinefb=pd.read_excel(\"online_douman_raw.xlsx\")\n",
    "#drop the first unwanted row\n",
    "raw_onlinefb=raw_onlinefb.drop([0], axis=0)\n",
    "\n",
    "#drop unwanted columns\n",
    "raw_onlinefb=raw_onlinefb.drop([ 'EndDate', 'Status', 'IPAddress','Finished', \n",
    "       'RecipientLastName', 'RecordedDate', 'ResponseId', 'RecipientFirstName', 'RecipientEmail', 'ExternalReference', \n",
    "                                'LocationLatitude',\n",
    "                                'LocationLongitude', 'UserLanguage',], axis=1)\n",
    "\n",
    "#drop the participants did not complete their survey\n",
    "\n",
    "raw_onlinefb=raw_onlinefb[raw_onlinefb['Progress']==100] \n",
    "distribution=['anonymous','qr']\n",
    "raw_onlinefb=raw_onlinefb.loc[raw_onlinefb['DistributionChannel'].isin(distribution)]\n",
    "\n",
    "#drop the cols of VP codes if they are empty\n",
    "raw_onlinefb.dropna(subset=['Q132_1'], inplace=True)\n",
    "#rename the code in Q132-1 which is for Gato14KO which has been wrongly filled as ANPE07B\n",
    "\n",
    "#columns that cover TICS QS\n",
    "df_tics=['Q2_1', 'Q2_2', 'Q2_3', 'Q2_4', 'Q2_5', 'Q111_1', 'Q111_2', 'Q111_3', 'Q111_4', 'Q111_5',\n",
    "         'Q76_1', 'Q76_2','Q76_3', 'Q76_5', 'Q110_1', 'Q110_2', 'Q110_3', 'Q110_4', \n",
    "         'Q110_5', 'Q77_1', 'Q77_2', 'Q77_3', 'Q77_4', 'Q77_5', 'Q112_1',  \n",
    "         'Q112_2', 'Q112_3', 'Q112_4', 'Q112_5', 'Q78_1', 'Q78_2','Q78_3', \n",
    "         'Q78_4', 'Q78_5', 'Q113_1', 'Q113_2', 'Q113_3', 'Q113_4',\n",
    "         'Q113_5', 'Q79_1', 'Q79_2', 'Q79_3', 'Q79_4', 'Q114_1','Q114_2', \n",
    "         'Q114_3', 'Q114_4', 'Q114_5', 'Q80_1',  'Q80_3',\n",
    "         'Q80_4', 'Q80_5', 'Q115_1', 'Q115_2', 'Q115_3', 'Q115_4',\n",
    "         'Q115_5']\n",
    "\n",
    "#There are three control question that \n",
    "#every VP need to have three of that in his/her DF\n",
    "control_qs=['Q76_4', 'Q79_5','Q80_2',]\n",
    "Control_1=[]\n",
    "Control_2=[]\n",
    "Control_3=[]\n",
    "for i in raw_onlinefb[control_qs[0]]:\n",
    "    if i==4:\n",
    "        a='True'\n",
    "        Control_1.append(a)\n",
    "    else:\n",
    "        \n",
    "        b='false'\n",
    "        Control_1.append(b)\n",
    "    \n",
    "for i in raw_onlinefb[control_qs[1]]:\n",
    "    if i==1:\n",
    "        a='True'\n",
    "        Control_2.append(a)\n",
    "    else:\n",
    "        \n",
    "        b='False'\n",
    "        Control_2.append(b)   \n",
    "for i in raw_onlinefb[control_qs[2]]:\n",
    "    if i==2:\n",
    "        a='True'\n",
    "        Control_3.append(a)\n",
    "    else:\n",
    "        \n",
    "        b='False'\n",
    "        Control_3.append(b)    \n",
    "\n",
    "#identifying the 9-subgroups of TICS\n",
    "#category and its questions\n",
    "#items covering work overload\n",
    "WOL=[1,4,44,54,17,27,38,50]\n",
    "#items covering social overload\n",
    "SO=[49,57,7,19,28,39]\n",
    "#items covering Pressure to Perform\n",
    "PP=[8,12,14,22,30,23,32,40,43]\n",
    "#items covering work discontent\n",
    "WD=[5,10,13,41,21,37,48,53]\n",
    "#items covering Excessive Demands at Work\n",
    "EDW=[3,47,20,24,35,55]\n",
    "#items covering Lack of Social Recognition\n",
    "LSR=[2,18,31,46]\n",
    "#items covering Social Tensions\n",
    "ST=[6,33,15,26,45,52]\n",
    "#items covering social isolation\n",
    "SI=[29,34,11,42,51,56]\n",
    "#chronic worring\n",
    "CW=[9,16,25,36]\n",
    "tics_num=[]\n",
    "for i in range(1,61):\n",
    "    n=i\n",
    "    if n in LSR:\n",
    "        n=str(n)+\"_\"+'LSR'\n",
    "        tics_num.append(n)\n",
    "    elif n in SO:\n",
    "        n=str(n)+\"_\"+'SO'\n",
    "        tics_num.append(n)\n",
    "    elif n in PP:\n",
    "        n=str(n)+\"_\"+'PP'\n",
    "        tics_num.append(n)\n",
    "    elif n in WD:\n",
    "        n=str(n)+\"_\"+'WD'\n",
    "        tics_num.append(n)\n",
    "    elif n in EDW:\n",
    "        n=str(n)+\"_\"+'EDW'\n",
    "        tics_num.append(n)\n",
    "    elif n in WOL:\n",
    "        n=str(n)+\"_\"+'WOL'\n",
    "        tics_num.append(n)\n",
    "    elif n in ST:\n",
    "        n=str(n)+\"_\"+'ST'\n",
    "        tics_num.append(n)\n",
    "    elif n in SI:\n",
    "        n=str(n)+\"_\"+'SI'\n",
    "        tics_num.append(n)\n",
    "    elif n in CW:\n",
    "        n=str(n)+\"_\"+'CW'\n",
    "        tics_num.append(n)\n",
    "        \n",
    "#Convert label of tic cols into numbers\n",
    "#with shoeing they belong to which subcategory of tics\n",
    "tics_dic=dict(zip(df_tics,tics_num))   \n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q2_1': '1_WOL', 'Q2_2': '2_LSR' ,'Q2_3': '3_EDW', 'Q2_4': '4_WOL',\n",
    "                                          'Q2_5': '5_WD',\n",
    " 'Q111_1': '6_ST', 'Q111_2': '7_SO', 'Q111_3': '8_PP', 'Q111_4': '9_CW', 'Q111_5': '10_WD', 'Q76_1': '11_SI', 'Q76_2': '12_PP', 'Q76_3': '13_WD',\n",
    " 'Q76_5': '14_PP',\n",
    " 'Q110_1': '15_ST', 'Q110_2': '16_CW', 'Q110_3': '17_WOL', 'Q110_4': '18_LSR', 'Q110_5': '19_SO', 'Q77_1': '20_EDW', 'Q77_2': '21_WD', 'Q77_3': '22_PP', 'Q77_4': '23_PP', 'Q77_5': '24_EDW', 'Q112_1': '25_CW', 'Q112_2': '26_ST', 'Q112_3': '27_WOL', 'Q112_4': '28_SO', 'Q112_5': '29_SI', 'Q78_1': '30_PP',\n",
    " 'Q78_2': '31_LSR',\n",
    " 'Q78_3': '32_PP',\n",
    " 'Q78_4': '33_ST', 'Q78_5': '34_SI', 'Q113_1': '35_EDW', 'Q113_2': '36_CW', 'Q113_3': '37_WD', 'Q113_4': '38_WOL', 'Q113_5': '39_SO', 'Q79_1': '40_PP', 'Q79_2': '41_WD',\n",
    " 'Q79_3': '42_SI', 'Q79_4': '43_PP', 'Q114_1': '44_WOL', 'Q114_2': '45_ST', 'Q114_3': '46_LSR', 'Q114_4': '47_EDW', 'Q114_5': '48_WD',\n",
    " 'Q80_1': '49_SO', 'Q80_3': '50_WOL', 'Q80_4': '51_SI', 'Q80_5': '52_ST',\n",
    " 'Q115_1': '53_WD', 'Q115_2': '54_WOL', 'Q115_3': '55_EDW', 'Q115_4': '56_SI', 'Q115_5': '57_SO'})\n",
    "raw_onlinefb['TICS']=raw_onlinefb[tics_num].sum(axis=1)\n",
    "raw_onlinefb['TICS_control1']=Control_1\n",
    "raw_onlinefb['TICS_control2']=Control_2\n",
    "raw_onlinefb['TICS_control3']=Control_3\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "\n",
    "#SVO data preparation\n",
    "#to calculate the social value orientation\n",
    "svo=['Q92_1','Q103_1','Q102_1','Q101_1','Q100_1','Q99_1','Q98_1','Q97_1','Q96_1',]\n",
    "svo_num=['svo_1','svo_2','svo_3','svo_4','svo_5','svo_6','svo_7','svo_8','svo_9']\n",
    "svo_dict=dict(zip(svo,svo_num))\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q92_1': 'svo_1',\n",
    " 'Q103_1': 'svo_2',\n",
    " 'Q102_1': 'svo_3',\n",
    " 'Q101_1': 'svo_4',\n",
    " 'Q100_1': 'svo_5',\n",
    " 'Q99_1': 'svo_6',\n",
    " 'Q98_1': 'svo_7',\n",
    " 'Q97_1': 'svo_8',\n",
    " 'Q96_1': 'svo_9'})\n",
    "#individualistic is H\n",
    "#Competitive Com\n",
    "# properative pro\n",
    "#the order of orientations in online qs of SVO\n",
    "#I-Com-pro\tCom-pro-I\tpro-I-Com\tI-Com-pro\tCom-pro-I\n",
    "#pro-I-Com\tpro-Com-I\tCom-I-pro\tI-pro-Com\n",
    "#0 means pro/1 means Com/ 2 means I\n",
    "#I-Com-pro\n",
    "#the empty rows are called NONE\n",
    "svo_1=[]\n",
    "for i in raw_onlinefb['svo_1']:\n",
    "    if i==1:\n",
    "        svo_q1='ind'\n",
    "        svo_1.append(svo_q1)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q1='Com'\n",
    "        svo_1.append(svo_q1)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q1='pro'\n",
    "        svo_1.append(svo_q1)  \n",
    "    else:\n",
    "        svo_q1='None'\n",
    "        svo_1.append(svo_q1)\n",
    "        \n",
    "#add the svo_1 col\n",
    "raw_onlinefb['svo_1']=svo_1\n",
    "#Com-pro-I       \n",
    "#svo_q2\n",
    "svo_2=[]\n",
    "for i in raw_onlinefb['svo_2']:\n",
    "    if i==1:\n",
    "        svo_q2='Com'\n",
    "        svo_2.append(svo_q2)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q2='pro'\n",
    "        svo_2.append(svo_q2)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q2='ind'\n",
    "        svo_2.append(svo_q2) \n",
    "    else:\n",
    "        svo_q2='None'\n",
    "        svo_2.append(svo_q2)\n",
    "        \n",
    "    \n",
    "        \n",
    "#add the svo_2 col\n",
    "raw_onlinefb['svo_2']=svo_2\n",
    "#pro-I-Com\n",
    "#svo_q3\n",
    "svo_3=[]\n",
    "for i in raw_onlinefb['svo_3']:\n",
    "    if i==1:\n",
    "        svo_q3='pro'\n",
    "        svo_3.append(svo_q3)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q3='ind'\n",
    "        svo_3.append(svo_q3)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q3='Com'\n",
    "        svo_3.append(svo_q3)\n",
    "    else:\n",
    "        svo_q3='None'\n",
    "        svo_3.append(svo_q3)\n",
    "        \n",
    "#add the svo_3 col\n",
    "raw_onlinefb['svo_3']=svo_3\n",
    "\n",
    "#I-Com-pro\n",
    "svo_4=[]\n",
    "for i in raw_onlinefb['svo_4']:\n",
    "    if i==1:\n",
    "        svo_q4='ind'\n",
    "        svo_4.append(svo_q4)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q4='Com'\n",
    "        svo_4.append(svo_q4)\n",
    "    \n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q4='pro'\n",
    "        svo_4.append(svo_q4) \n",
    "    else:\n",
    "        svo_q4='None'\n",
    "        svo_4.append(svo_q4)\n",
    "        \n",
    "#add the svo_4 col\n",
    "raw_onlinefb['svo_4']=svo_4  \n",
    "\n",
    "\n",
    "#Com-pro-I       \n",
    "#svo_q5\n",
    "svo_5=[]\n",
    "for i in raw_onlinefb['svo_5']:\n",
    "    if i==1:\n",
    "        svo_q5='Com'\n",
    "        svo_5.append(svo_q5)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q5='pro'\n",
    "        svo_5.append(svo_q5)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q5='ind'\n",
    "        svo_5.append(svo_q5) \n",
    "    else:\n",
    "        svo_q5='None'\n",
    "        svo_5.append(svo_q5)\n",
    "        \n",
    "#add the svo_5 col\n",
    "raw_onlinefb['svo_5']=svo_5\n",
    "\n",
    "\n",
    "#pro-I-Com\n",
    "#svo_q6\n",
    "svo_6=[]\n",
    "for i in raw_onlinefb['svo_6']:\n",
    "    if i==1:\n",
    "        svo_q6='pro'\n",
    "        svo_6.append(svo_q6)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q6='ind'\n",
    "        svo_6.append(svo_q6)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q6='Com'\n",
    "        svo_6.append(svo_q6)\n",
    "    else:\n",
    "        svo_q6='None'\n",
    "        svo_6.append(svo_q6)\n",
    "        \n",
    "#add the svo_6 col\n",
    "raw_onlinefb['svo_6']=svo_6\n",
    "\n",
    "#pro-Com-I\n",
    "#svo_7\n",
    "svo_7=[]\n",
    "for i in raw_onlinefb['svo_7']:\n",
    "    if i==1:\n",
    "        svo_q7='pro'\n",
    "        svo_7.append(svo_q7)\n",
    "    elif i==2:\n",
    "        svo_q7='Com'\n",
    "        svo_7.append(svo_q7)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q7='ind'\n",
    "        svo_7.append(svo_q7)\n",
    "    else:\n",
    "        svo_q7='None'\n",
    "        svo_7.append(svo_q7)\n",
    "        \n",
    "#add the svo_7 col\n",
    "raw_onlinefb['svo_7']=svo_7\n",
    "\n",
    "#Com-I-pro\n",
    "#svo_8\n",
    "svo_8=[]\n",
    "for i in raw_onlinefb['svo_8']:\n",
    "    if i==1:\n",
    "        svo_q8='Com'\n",
    "        svo_8.append(svo_q8)\n",
    "    elif i==2:\n",
    "        svo_q8='ind'\n",
    "        svo_8.append(svo_q8)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q8='pro'\n",
    "        svo_8.append(svo_q8)\n",
    "    else:\n",
    "        svo_q8='None'\n",
    "        svo_8.append(svo_q8)\n",
    "#add the svo_8 col\n",
    "raw_onlinefb['svo_8']=svo_8\n",
    "\n",
    "#I-pro-Com\n",
    "#svo_9\n",
    "svo_9=[]\n",
    "for i in raw_onlinefb['svo_9']:\n",
    "    if i==1:\n",
    "        svo_q9='ind'\n",
    "        svo_9.append(svo_q9)\n",
    "    elif i==2:\n",
    "        svo_q9='pro'\n",
    "        svo_9.append(svo_q9)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q9='Com'\n",
    "        svo_9.append(svo_q9)\n",
    "    else:\n",
    "        svo_q9='None'\n",
    "        svo_9.append(svo_q9)\n",
    "\n",
    "#add the svo_9 col\n",
    "raw_onlinefb['svo_9']=svo_9\n",
    "#i need to convert the number of times a word is there\n",
    "pro = ['pro']\n",
    "ind = ['ind']\n",
    "com = ['Com']\n",
    "raw_onlinefb[\"prosocial\"] = raw_onlinefb.isin(pro).sum(axis='columns')\n",
    "raw_onlinefb[\"individualistic\"] = raw_onlinefb.isin(ind).sum(axis='columns')\n",
    "raw_onlinefb[\"competetive\"] = raw_onlinefb.isin(com).sum(axis='columns')\n",
    "#create a column to clarify fainall y hwre every one should be in the category \n",
    "# if someone has 6 or more times made a decision you choose that category of SVO for the person\n",
    "conditions = [\n",
    "    (raw_onlinefb[\"prosocial\"] >= 6),\n",
    "    (raw_onlinefb[\"individualistic\"] >=6) , (raw_onlinefb[\"competetive\"] >= 6)]\n",
    "values = [\"prosocial\", \"individualistic\", \"competetive\"]\n",
    "raw_onlinefb['SVO_Final'] = np.select(conditions, values)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "#to prepare data for analysis of BIS/BAS\n",
    "#Q75_1\tQ75_2\tQ75_3\tQ75_4\t\n",
    "#Q75_5\tQ75_6\tQ116_1\tQ116_2\tQ116_3\tQ116_4\tQ116_5\tQ116_6\n",
    "#Q117_1\tQ117_2\tQ117_3\tQ117_4\tQ117_5\n",
    "#Q117_6\tQ118_1\tQ118_2\tQ118_3\tQ118_4\tQ118_5\tQ118_6\n",
    "#raw_onlinefb.columns.to_list()\n",
    "\n",
    "#Items 1, 3-21, 23-24 are reverse-scored.\n",
    "#Items 2 and 22 are scored normally.\n",
    " \n",
    "#4 Subscales: Behavioral Inhibition System (BIS), Behavioral Activation System (BAS) Drive, BAS Fun Seeking, BAS Reward Responsiveness\n",
    "#BIS:  items 2, 8, 13, 16, 19, 22, 24\n",
    "#BAS Drive:  items 3, 9, 12, 21 \n",
    "#BAS Fun Seeking:  items 5, 10, 15, 20 \n",
    "#BAS Reward Responsiveness:  items 4, 7, 14, 18, 23\n",
    "bis_bas=['Q75_1','Q75_2','Q75_3','Q75_4','Q75_5','Q75_6','Q116_1',\n",
    "         'Q116_2','Q116_3',\n",
    "         'Q116_4','Q116_5','Q116_6',\n",
    "         'Q117_1','Q117_2','Q117_3','Q117_4',\n",
    "         'Q117_5','Q117_6','Q118_1','Q118_2','Q118_3','Q118_4','Q118_5',\n",
    "         'Q118_6']\n",
    "BIS_Total=[2, 8, 13, 16, 19, 22, 24]\n",
    "BAS_total=[3, 4, 5, 7, 9, 10, 12, 14, 15, 18, 20, 21, 23]\n",
    "BAS_drive=[3, 9, 12, 21 ]\n",
    "BAS_FUN=[5, 10, 15, 20]\n",
    "BAS_rewards=[4, 7, 14, 18, 23]\n",
    "Filler=[1,6,11,17]\n",
    "BIS_BAS_num=[]\n",
    "#to find qs excist in all and also to create a groupe separately\n",
    "BAS_group=[BAS_total,BAS_FUN,BAS_rewards]\n",
    "\n",
    "for i in range(1,25):\n",
    "\n",
    "    if i in (BAS_total and BAS_FUN):\n",
    "        i=str(i)+\"_\"+'total_fun'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in (BAS_total and BAS_rewards):\n",
    "        i=str(i)+\"_\"+'total_reward'\n",
    "        BIS_BAS_num.append(i)    \n",
    "\n",
    "    elif i in BAS_drive:\n",
    "        i=str(i)+\"_\"+'BAS_drive'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in BAS_FUN:\n",
    "        i=str(i)+\"_\"+'BAS_FUN'\n",
    "        BIS_BAS_num.append(i)    \n",
    "    elif i in BAS_rewards:\n",
    "        i=str(i)+\"_\"+'BAS_rewards'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in BIS_Total:\n",
    "        i=str(i)+\"_\"+'BIS_Total'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in Filler:\n",
    "        i=str(i)+\"_\"+'Filler'\n",
    "        BIS_BAS_num.append(i)      \n",
    "\n",
    "#make a dic to re_label the raw col names from raw data for BIS/BAS test\n",
    "#bisbas_dic=dict(zip(bis_bas,BIS_BAS_num))   \n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q75_1': '1_Filler','Q75_2': '2_BIS_Total','Q75_3': '3_BAS_drive',\n",
    "                                          'Q75_4': '4_total_reward','Q75_5': '5_total_fun','Q75_6': '6_Filler',\n",
    " 'Q116_1': '7_total_reward', 'Q116_2': '8_BIS_Total', 'Q116_3': '9_BAS_drive', 'Q116_4': '10_total_fun', 'Q116_5': '11_Filler', 'Q116_6': '12_BAS_drive', 'Q117_1': '13_BIS_Total', 'Q117_2': '14_total_reward',\n",
    " 'Q117_3': '15_total_fun', 'Q117_4': '16_BIS_Total', 'Q117_5': '17_Filler', 'Q117_6': '18_total_reward', 'Q118_1': '19_BIS_Total',\n",
    " 'Q118_2': '20_total_fun', 'Q118_3': '21_BAS_drive', 'Q118_4': '22_BIS_Total', 'Q118_5': '23_total_reward', 'Q118_6': '24_BIS_Total'})\n",
    "\n",
    "#there are some reverse scoring\n",
    "raw_onlinefb['2_BIS_Total'] = raw_onlinefb['2_BIS_Total'].map({1:4, 2:3, 3:2, 4:1})\n",
    "raw_onlinefb['22_BIS_Total'] = raw_onlinefb['22_BIS_Total'].map({1:4, 2:3, 3:2, 4:1})    \n",
    "\n",
    "#adding columns based on total BIS or BAs and also sub-factors\n",
    "BIS_Total_qs=['2_BIS_Total','8_BIS_Total','13_BIS_Total','16_BIS_Total','19_BIS_Total','22_BIS_Total','24_BIS_Total']\n",
    "raw_onlinefb['BIS_Total_qs']=raw_onlinefb[BIS_Total_qs].sum(axis=1)\n",
    "\n",
    "BAS_total_qs=['3_BAS_drive','4_total_reward','5_total_fun','7_total_reward','9_BAS_drive','10_total_fun',\n",
    "           '12_BAS_drive','14_total_reward','15_total_fun','18_total_reward','20_total_fun','21_BAS_drive','23_total_reward']\n",
    "raw_onlinefb['BAS_total_qs']=raw_onlinefb[BAS_total_qs].sum(axis=1)\n",
    "\n",
    "BAS_drive=['3_BAS_drive','9_BAS_drive','12_BAS_drive','21_BAS_drive']\n",
    "raw_onlinefb['BAS_drive']=raw_onlinefb[BAS_drive].sum(axis=1)\n",
    "\n",
    "BAS_FUN=['5_total_fun','10_total_fun','15_total_fun','20_total_fun']\n",
    "raw_onlinefb['BAS_FUN']=raw_onlinefb[BAS_FUN].sum(axis=1)\n",
    "\n",
    "BAS_rewards=['4_total_reward','7_total_reward','14_total_reward','18_total_reward','23_total_reward']\n",
    "raw_onlinefb['BAS_rewards']=raw_onlinefb[BAS_rewards].sum(axis=1)\n",
    "\n",
    "Filler=['1_Filler','6_Filler','11_Filler','17_Filler']\n",
    "raw_onlinefb['Filler']=raw_onlinefb[Filler].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "#to prepare BIS_15 qs for data analysis\n",
    "#cols that include the bis 15 qs\n",
    "Bis_15_raw=['Q82_1','Q82_2','Q82_3','Q82_4','Q82_5','Q85_1','Q85_2','Q85_3','Q85_4','Q85_5','Q86_1',\n",
    "'Q86_2','Q86_3','Q86_4','Q86_5']\n",
    "#non-planning impulsiveness\n",
    "NPI=[1,5,7,8,15]\n",
    "#motor impulsivity\n",
    "MI=[2,10,12,13,9]\n",
    "#Attention-based impulsivity.\n",
    "ABI=[14,6,4,3,11]\n",
    "#make a list of BIS qs\n",
    "BIS_15_lis=[]\n",
    "for i in range(1,16):\n",
    "    if i in NPI:\n",
    "        i=str(i)+\"-\"+'BIS-15-NPI'\n",
    "        BIS_15_lis.append(i)\n",
    "    if i in MI:\n",
    "        i=str(i)+\"-\"+'BIS-15-MI'\n",
    "        BIS_15_lis.append(i)\n",
    "    if i in ABI:\n",
    "        i=str(i)+\"-\"+'BIS-15-ABI'\n",
    "        BIS_15_lis.append(i)\n",
    "\n",
    "#need to synch raw with edited col names\n",
    "#bis15_dic=dict(zip(Bis_15_raw,BIS_15_lis))  \n",
    "#rename the bis15 cols\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q82_1': '1-BIS-15-NPI',\n",
    " 'Q82_2': '2-BIS-15-MI',\n",
    " 'Q82_3': '3-BIS-15-ABI', 'Q82_4': '4-BIS-15-ABI', 'Q82_5': '5-BIS-15-NPI', 'Q85_1': '6-BIS-15-ABI', 'Q85_2': '7-BIS-15-NPI', 'Q85_3': '8-BIS-15-NPI','Q85_4': '9-BIS-15-MI',\n",
    "'Q85_5': '10-BIS-15-MI', 'Q86_1': '11-BIS-15-ABI',\n",
    " 'Q86_2': '12-BIS-15-MI', 'Q86_3': '13-BIS-15-MI', 'Q86_4': '14-BIS-15-ABI', 'Q86_5': '15-BIS-15-NPI'})\n",
    "\n",
    "#there are qs need to be reversed for scoring\n",
    "#questions should be reversed 13,15,1,7,4\n",
    "raw_onlinefb['13-BIS-15-MI'] = raw_onlinefb['13-BIS-15-MI'].map({1:4, 2:3, 3:2, 4:1})\n",
    "raw_onlinefb['15-BIS-15-NPI']= raw_onlinefb['15-BIS-15-NPI'].map({1:4, 2:3, 3:2, 4:1})    \n",
    "raw_onlinefb['1-BIS-15-NPI'] = raw_onlinefb['1-BIS-15-NPI'].map({1:4, 2:3, 3:2, 4:1})\n",
    "raw_onlinefb['7-BIS-15-NPI'] = raw_onlinefb['7-BIS-15-NPI'].map({1:4, 2:3, 3:2, 4:1})   \n",
    "raw_onlinefb['4-BIS-15-ABI'] = raw_onlinefb['4-BIS-15-ABI'].map({1:4, 2:3, 3:2, 4:1})\n",
    "\n",
    "#make new cols for each subcategory of BIS15 and also a total one\n",
    "bis_15_mi=['2-BIS-15-MI','9-BIS-15-MI','10-BIS-15-MI','12-BIS-15-MI','13-BIS-15-MI']\n",
    "raw_onlinefb['BIS_15_MI']=raw_onlinefb[bis_15_mi].sum(axis=1)\n",
    "\n",
    "bis_15_npi=['1-BIS-15-NPI','5-BIS-15-NPI','7-BIS-15-NPI','8-BIS-15-NPI','15-BIS-15-NPI']\n",
    "raw_onlinefb['BIS_15_NPI']=raw_onlinefb[bis_15_npi].sum(axis=1)\n",
    "\n",
    "bis_15_abi=['3-BIS-15-ABI','4-BIS-15-ABI','6-BIS-15-ABI','11-BIS-15-ABI','14-BIS-15-ABI']\n",
    "raw_onlinefb['BIS_15_ABI']=raw_onlinefb[bis_15_abi].sum(axis=1)\n",
    "\n",
    "#create a col form all 15 items of BiS-15\n",
    "BIS_15_total=['2-BIS-15-MI','9-BIS-15-MI','10-BIS-15-MI','12-BIS-15-MI','13-BIS-15-MI',\n",
    "       '1-BIS-15-NPI','5-BIS-15-NPI','7-BIS-15-NPI','8-BIS-15-NPI','15-BIS-15-NPI',\n",
    "       '3-BIS-15-ABI','4-BIS-15-ABI','6-BIS-15-ABI','11-BIS-15-ABI','14-BIS-15-ABI']\n",
    "raw_onlinefb['BIS_15_total']=raw_onlinefb[BIS_15_total].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "#BFI data prepration\n",
    "BFI_Raw=['Q135_1', 'Q135_2', 'Q135_3', 'Q135_4', 'Q135_5', 'Q136_1',\n",
    "         'Q136_2','Q136_3','Q136_4','Q136_5']\n",
    "#five dimension of BFI\n",
    "N=[4,9]\n",
    "E=[1,6]\n",
    "O=[5,10]\n",
    "A=[2,7]\n",
    "C=[3,8]\n",
    "#list of BFI\n",
    "BFI_list=[]\n",
    "for i in range(1,11):\n",
    "    if i in N:\n",
    "        i=str(i)+\"-\"+'BFI_N'\n",
    "        BFI_list.append(i)\n",
    "    if i in E:\n",
    "        i=str(i)+\"-\"+'BFI_E'\n",
    "        BFI_list.append(i)\n",
    "    if i in O:\n",
    "        i=str(i)+\"-\"+'BFI_O'\n",
    "        BFI_list.append(i)        \n",
    "    if i in A:\n",
    "        i=str(i)+\"-\"+'BFI_A'\n",
    "        BFI_list.append(i)\n",
    "    if i in C:\n",
    "        i=str(i)+\"-\"+'BFI_C'\n",
    "        BFI_list.append(i)\n",
    "\n",
    "#need to synch raw with edited col names\n",
    "BFI_dic=dict(zip(BFI_Raw,BFI_list))  \n",
    "\n",
    "#rename the bis15 cols\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q135_1': '1-BFI_E', 'Q135_2': '2-BFI_A','Q135_3': '3-BFI_C',                                           'Q135_4': '4-BFI_N',\n",
    "'Q135_5': '5-BFI_O','Q136_1': '6-BFI_E', 'Q136_2': '7-BFI_A', 'Q136_3': '8-BFI_C','Q136_4': '9-BFI_N', 'Q136_5': '10-BFI_O'})\n",
    "\n",
    "#BFI- questions should be reversed 1,3,4,5,7\n",
    "raw_onlinefb['1-BFI_E'] = raw_onlinefb['1-BFI_E'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['3-BFI_C'] = raw_onlinefb['3-BFI_C'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['4-BFI_N'] = raw_onlinefb['4-BFI_N'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['5-BFI_O'] = raw_onlinefb['5-BFI_O'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['7-BFI_A'] = raw_onlinefb['7-BFI_A'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "#to calculate mean of each dimension\n",
    "# N=[4,9]\n",
    "# E=[1,6]\n",
    "# O=[5,10]\n",
    "# A=[2,7]\n",
    "# C=[3,8]\n",
    "#create new cols for calclualtion of each dim of BFI (mean)\n",
    "bfi_n=['4-BFI_N','9-BFI_N']\n",
    "raw_onlinefb['BFI_N']=raw_onlinefb[bfi_n].mean(axis=1)\n",
    "bfi_e=['1-BFI_E','6-BFI_E']\n",
    "raw_onlinefb['BFI_E']=raw_onlinefb[bfi_e].mean(axis=1)\n",
    "bfi_o=['5-BFI_O','10-BFI_O']\n",
    "raw_onlinefb['BFI_O']=raw_onlinefb[bfi_o].mean(axis=1)\n",
    "bfi_a=['2-BFI_A','7-BFI_A']\n",
    "raw_onlinefb['BFI_A']=raw_onlinefb[bfi_a].mean(axis=1)\n",
    "bfi_c=['3-BFI_C','8-BFI_C']\n",
    "raw_onlinefb['BFI_C']=raw_onlinefb[bfi_c].mean(axis=1)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "\n",
    "#CRT-there are three questions with right answer \n",
    "crt_ball=5\n",
    "crt_machine=5\n",
    "crt_lake=47\n",
    "#raw col names\n",
    "crt_raw=['Q88_1','Q89_1','Q90_1']\n",
    "#rename the raw cols\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q88_1':'CRT_ball','Q89_1':'CRT_machine','Q90_1':'CRT_lake'})\n",
    "#create empty list for three crts\n",
    "crt_ball=[]\n",
    "crt_machine=[]\n",
    "crt_lake=[]\n",
    "#there are different kind of answers for each crt in raw data\n",
    "crt_ball_answers=['5','0,05','5 cent']\n",
    "crt_machine_answers=['5','5 Minuten']\n",
    "crt_lake_answers=['47','47 Tage']\n",
    "for i in raw_onlinefb.CRT_ball:\n",
    "    if i in crt_ball_answers:\n",
    "        i='True'\n",
    "        crt_ball.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_ball.append(i)\n",
    "for i in raw_onlinefb.CRT_machine:\n",
    "    if i in crt_machine_answers:\n",
    "        i='True'\n",
    "        crt_machine.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_machine.append(i)\n",
    "for i in raw_onlinefb.CRT_machine:\n",
    "    if i in crt_machine_answers:\n",
    "        i='True'\n",
    "        crt_machine.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_machine.append(i)\n",
    "for i in raw_onlinefb.CRT_lake:\n",
    "    if i in crt_lake_answers:\n",
    "        i='True'\n",
    "        crt_lake.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_lake.append(i)\n",
    "#creating new graphs with results of for loop for each crt question\n",
    "raw_onlinefb['crt_ball']=crt_ball\n",
    "raw_onlinefb['crt_machine']=crt_lake\n",
    "raw_onlinefb['crt_lake']=crt_lake\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#to prepare the STAI qs for data analysis\n",
    "#For STAI, there are two set of qs, each 20.\n",
    "\n",
    "#the name of stai cols in raw data\n",
    "stais_raw=['Q67_1', 'Q67_2', 'Q67_3', 'Q67_4', 'Q67_5', 'Q119_1', 'Q119_2',\n",
    " 'Q119_3', 'Q119_4', 'Q119_5', 'Q120_1', 'Q120_2', 'Q120_3', 'Q120_4', 'Q120_5',\n",
    " 'Q121_1', 'Q121_2', 'Q121_3', 'Q121_4', 'Q121_5', 'Q71_1', 'Q71_2', 'Q71_3',\n",
    " 'Q71_4', 'Q71_5', 'Q122_1', 'Q122_2', 'Q122_3', 'Q122_4', 'Q122_5', 'Q123_1', 'Q123_2',\n",
    " 'Q123_3', 'Q123_4', 'Q123_5', 'Q124_1', 'Q124_2', 'Q124_3', 'Q124_4', 'Q124_5']\n",
    "nums=[]\n",
    "for i in range(1,41):\n",
    "    i=str(i)+\"-\"+'stai'\n",
    "    nums.append(i)\n",
    "    \n",
    "#need to synch raw with nums\n",
    "stai_dic=dict(zip(stais_raw,nums)) \n",
    "#rename the cols from raw to created ones\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q67_1': '1-stai', 'Q67_2': '2-stai', 'Q67_3': '3-stai', 'Q67_4': '4-stai', 'Q67_5': '5-stai',\n",
    " 'Q119_1': '6-stai', 'Q119_2': '7-stai', 'Q119_3': '8-stai', 'Q119_4': '9-stai', 'Q119_5': '10-stai', 'Q120_1': '11-stai', 'Q120_2': '12-stai',\n",
    " 'Q120_3': '13-stai', 'Q120_4': '14-stai', 'Q120_5': '15-stai', 'Q121_1': '16-stai', 'Q121_2': '17-stai', 'Q121_3': '18-stai',\n",
    " 'Q121_4': '19-stai', 'Q121_5': '20-stai', 'Q71_1': '21-stai', 'Q71_2': '22-stai', 'Q71_3': '23-stai', 'Q71_4': '24-stai',\n",
    "                        'Q71_5': '25-stai', 'Q122_1': '26-stai','Q122_2': '27-stai', 'Q122_3': '28-stai', 'Q122_4': '29-stai', 'Q122_5': '30-stai',\n",
    " 'Q123_1': '31-stai', 'Q123_2': '32-stai', 'Q123_3': '33-stai', 'Q123_4': '34-stai', 'Q123_5': '35-stai', 'Q124_1': '36-stai',\n",
    " 'Q124_2': '37-stai', 'Q124_3': '38-stai', 'Q124_4': '39-stai', 'Q124_5': '40-stai'})\n",
    "#to reverse the specific questions\n",
    "##There are many questions reverted: Y1=[1,2,5,8,10,11,15,16,19,20] for Y2=[21,23,26,27,30,33,34,36,39]\n",
    "reversed_qs=['1-stai', '2-stai', '5-stai', '8-stai', '10-stai', '11-stai', '15-stai',\n",
    " '16-stai', '19-stai', '20-stai', '21-stai', '23-stai', '26-stai', '27-stai', '30-stai',\n",
    "             '33-stai', '34-stai','36-stai','39-stai']\n",
    "#reverse the answers\n",
    "raw_onlinefb[['1-stai', '2-stai', '5-stai', '8-stai', '10-stai', '11-stai', '15-stai',\n",
    " '16-stai', '19-stai', '20-stai', '21-stai', '23-stai', '26-stai', '27-stai', '30-stai',\n",
    "             '33-stai', '34-stai','36-stai','39-stai']] =raw_onlinefb[['1-stai', '2-stai', '5-stai', '8-stai', '10-stai', '11-stai', '15-stai',\n",
    " '16-stai', '19-stai', '20-stai', '21-stai', '23-stai', '26-stai', '27-stai', '30-stai',\n",
    "             '33-stai', '34-stai','36-stai','39-stai']].replace({1:4, 2:3, 3:2, 4:1})\n",
    "#calculate score of each participant for STAI\n",
    "raw_onlinefb['stai_total']=raw_onlinefb[reversed_qs].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "#to prepare raw data from SDS 17\n",
    "raw_sds=['Q33', 'Q65', 'Q49', 'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60', 'Q61', 'Q62', 'Q63', 'Q64']\n",
    "# new col names\n",
    "sds_num=[]\n",
    "for i in range(1,18):\n",
    "    i=str(i)+\"-\"+'SDS17'\n",
    "    sds_num.append(i)\n",
    "#need to synch raw with nums\n",
    "sds17_dic=dict(zip(raw_sds,sds_num)) \n",
    "#rename the cols from raw to created ones\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q33': '1-SDS17', 'Q65': '2-SDS17', 'Q49': '3-SDS17', 'Q51': '4-SDS17',\n",
    "                                          'Q52': '5-SDS17', 'Q53': '6-SDS17', 'Q54': '7-SDS17', 'Q55': '8-SDS17', \n",
    "                                          'Q56': '9-SDS17', 'Q57': '10-SDS17',\n",
    " 'Q58': '11-SDS17', 'Q59': '12-SDS17', 'Q60': '13-SDS17','Q61': '14-SDS17', \n",
    " 'Q62': '15-SDS17', 'Q63': '16-SDS17', 'Q64': '17-SDS17'})\n",
    "#there is no reverse scoring in SDS17 but i should only have the sum of 1 in raw data which means True\n",
    "sds_cols=['1-SDS17', '2-SDS17', '3-SDS17', '4-SDS17', '5-SDS17', '6-SDS17', '7-SDS17', '8-SDS17', '9-SDS17', '10-SDS17',\n",
    " '11-SDS17', '12-SDS17', '13-SDS17', '14-SDS17', '15-SDS17', '16-SDS17','17-SDS17']\n",
    "\n",
    "raw_onlinefb['sds_total']=raw_onlinefb[raw_onlinefb[sds_cols]==1].sum(axis=1)         \n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "#to prepare raw data of MEQ\n",
    "# MEQ scoring:\n",
    "# Q1->1-6\n",
    "# Q2->4-1\n",
    "# Q3->1-5\n",
    "# Q4->1-5\n",
    "# Q5->1-4\n",
    "meq_ls=['Q12', 'Q13', 'Q14', 'Q15', 'Q16']\n",
    "#change the name of cols\n",
    "meq_cols=['meq-1','meq-2','meq-3','meq-4','meq-5']\n",
    "#need to synch raw with nums\n",
    "meq_dict=dict(zip(meq_ls,meq_cols))\n",
    "#rename the cols from raw to created ones\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q12': 'meq-1', 'Q13': 'meq-2', 'Q14': 'meq-3', 'Q15': 'meq-4','Q16':'meq-5'})\n",
    "#teh second q should be reversed\n",
    "raw_onlinefb['meq-2'] = raw_onlinefb['meq-2'].map({4:1, 3:2, 2:3, 1:4})\n",
    "raw_onlinefb['meq_total']=raw_onlinefb[['meq-1','meq-2','meq-3','meq-4','meq-5']].sum(axis=1)\n",
    "#the higher the number more ecening person\n",
    "\n",
    "#to prepare escale\n",
    "raw_cols=['Q138_1', 'Q138_2', 'Q138_3', 'Q138_4', 'Q138_5', 'Q140_1', 'Q140_2', 'Q140_3', 'Q140_4','Q140_5', 'Q141_1',\n",
    " 'Q141_2', 'Q141_3', 'Q141_4', 'Q141_5', 'Q142_1', 'Q142_2', 'Q142_3', 'Q142_4', 'Q142_5', 'Q143_1', 'Q143_2','Q143_3', \n",
    "          'Q143_4', 'Q143_5']\n",
    "# Factor 1- cognitive-sensitivity [CS] 14,17,19,24,21\n",
    "# Factor 2- emotional-sensitivity [ES]1,2, 5, 7, 9, 11\n",
    "# Factor 3- emotional-concern [EC]10, 15, 18, 20 (negatively poled), 22, 23, 25 \n",
    "# Factor 4- cognitive-concern[CC] 3, 4, 6, 8, 13\n",
    "cs=[14,17,19,24,21]\n",
    "es=[1,2, 5, 7, 9, 11]\n",
    "ec=[10, 15, 18, 20, 22, 23, 25 ]\n",
    "cc=[3, 4, 6, 8, 13]\n",
    "e_scale=[]\n",
    "for i in range(1,26):\n",
    "    if i in cs:\n",
    "        i=str(i)+\"-\"+'escale_cs'\n",
    "        e_scale.append(i)\n",
    "    if i in es:\n",
    "        i=str(i)+\"-\"+'escale_es'\n",
    "        e_scale.append(i)\n",
    "    if i in ec:\n",
    "        i=str(i)+\"-\"+'escale_ec'\n",
    "        e_scale.append(i)\n",
    "    if i in cc:\n",
    "        i=str(i)+\"-\"+'escale_cc'\n",
    "        e_scale.append(i)\n",
    "        \n",
    "escale_dict=dict(zip(raw_cols,e_scale))        \n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q138_1': '1-escale_es', 'Q138_2': '2-escale_es', \n",
    "                                          'Q138_3': '3-escale_cc', 'Q138_4': '4-escale_cc', 'Q138_5': '5-escale_es', 'Q140_1': '6-escale_cc',\n",
    " 'Q140_2': '7-escale_es', 'Q140_3': '8-escale_cc','Q140_4': '9-escale_es', 'Q140_5': '10-escale_ec', \n",
    "                                          'Q141_1': '11-escale_es', 'Q141_2': '13-escale_cc',\n",
    " 'Q141_3': '14-escale_cs', 'Q141_4': '15-escale_ec', 'Q141_5': '17-escale_cs', 'Q142_1': '18-escale_ec',\n",
    "                                          'Q142_2': '19-escale_cs', 'Q142_3': '20-escale_ec',\n",
    " 'Q142_4': '21-escale_cs', 'Q142_5': '22-escale_ec', 'Q143_1': '23-escale_ec', 'Q143_2': '24-escale_cs',\n",
    "                                          'Q143_3': '25-escale_ec'})\n",
    "raw_onlinefb['20-escale_ec'] = raw_onlinefb['20-escale_ec'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['e-scale_total']=raw_onlinefb[e_scale].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "#to remove the empty cols of avatar test and bringing all under each other\n",
    "# i need to add 2 to empty cells in col Q500\n",
    "# then only for rows of q500 that are 2 and if 1_Q194_First Click is empty \n",
    "#remove the cols from 1_Q194_First Click to 1_Q196_First Click\n",
    "\n",
    "#add 2 to Q500\n",
    "raw_onlinefb.Q500.fillna(2,inplace=True)\n",
    " \n",
    "\n",
    "# Replacing empty string with np.NaN\n",
    "import numpy as np\n",
    "raw_onlinefb = raw_onlinefb.replace('', np.nan)\n",
    "df_left=raw_onlinefb[(~raw_onlinefb['1_Q194_First Click'].isnull())&(raw_onlinefb['Q500']==2)]\n",
    "#remove the unwanted cols from df_left\n",
    "\n",
    "df_left.drop(columns=df_left.loc[:,'1_Q196_First Click':'64_Q193'], axis=1,inplace=True)\n",
    "#creating two separated dfs to get rid of duplicate cols\n",
    "df_right=raw_onlinefb[(raw_onlinefb['1_Q194_First Click'].isnull())&(raw_onlinefb['Q500']==2)]\n",
    "\n",
    "#remove the unwanted cols from df_right\n",
    "df_right.drop(columns=df_right.loc[:,'Q500_1_TEXT':'64_Q175'], axis=1,inplace=True)\n",
    "\n",
    "#df_left name of cols \n",
    "col_df_left=df_left.loc[:,'1_Q194_First Click':'64_Q175'].columns.to_list()\n",
    "#df_right name of cols\n",
    "col_df_right=df_right.loc[:,'1_Q196_First Click':'64_Q193'].columns.to_list()\n",
    "#make dict for both cols names\n",
    "dict_col_name=dict(zip(col_df_right,col_df_left))  \n",
    "#rename the df_right specific col names by df_left col names\n",
    "df_right=df_right.rename(dict_col_name,axis=1)\n",
    "#add the information which order of perspective they did: left or right\n",
    "df_left['perspective-order']='left'\n",
    "df_right['perspective-order']='right'\n",
    "#encode the right and wrong questions\n",
    "#for now I coded not answered 'NONE'qs as wrong answered question\n",
    "#left order 5=stimmt nicht überein 4=stimmt überein \n",
    "#right order 4=stimmt nicht überein , 5=stimmt überein \n",
    "#questions and correct answers\n",
    "\n",
    "#question_left_5 as correct answer\n",
    "ql5c=['1_Q175','5_Q175','7_Q175','8_Q175','13_Q175','14_Q175','18_Q175','19_Q175','21_Q175','22_Q175','23_Q175','30_Q175','31_Q175','35_Q175'\n",
    ",'36_Q175','38_Q175','39_Q175','40_Q175','41_Q175','43_Q175','46_Q175','48_Q175','51_Q175','52_Q175','53_Q175','55_Q175','56_Q175','57_Q175',\n",
    "'59_Q175','60_Q175','63_Q175','64_Q175']\n",
    "df_left[ql5c]= np.where(df_left[ql5c]==5, 0, 1)\n",
    "#question_right_4 as correct answer\n",
    "qr4c=['1_Q175','5_Q175','7_Q175','8_Q175','13_Q175','14_Q175','18_Q175','19_Q175','21_Q175','22_Q175','23_Q175','30_Q175','31_Q175','35_Q175'\n",
    ",'36_Q175','38_Q175','39_Q175','40_Q175','41_Q175','43_Q175','46_Q175','48_Q175','51_Q175','52_Q175','53_Q175','55_Q175','56_Q175','57_Q175',\n",
    "'59_Q175','60_Q175','63_Q175','64_Q175']\n",
    "df_right[qr4c]= np.where(df_right[qr4c]==4, 0, 1)\n",
    "#question_left_4 as correct answer\n",
    "ql4c=['2_Q175','3_Q175','4_Q175','6_Q175','9_Q175','10_Q175','11_Q175','12_Q175','15_Q175','16_Q175','17_Q175','20_Q175','24_Q175','25_Q175'\n",
    ",'26_Q175','27_Q175','28_Q175','29_Q175','32_Q175','33_Q175','34_Q175','37_Q175','42_Q175','44_Q175','45_Q175','47_Q175','49_Q175','50_Q175',\n",
    "'54_Q175','58_Q175','61_Q175','62_Q175']\n",
    "df_left[ql4c]= np.where(df_left[ql4c]==4, 0, 1)\n",
    "#question_right_5 as correct answer\n",
    "qr5c=['2_Q175','3_Q175','4_Q175','6_Q175','9_Q175','10_Q175','11_Q175','12_Q175','15_Q175','16_Q175','17_Q175','20_Q175','24_Q175','25_Q175'\n",
    ",'26_Q175','27_Q175','28_Q175','29_Q175','32_Q175','33_Q175','34_Q175','37_Q175','42_Q175','44_Q175','45_Q175','47_Q175','49_Q175','50_Q175',\n",
    "'54_Q175','58_Q175','61_Q175','62_Q175']\n",
    "df_right[qr5c]= np.where(df_right[qr5c]==5, 0, 1)\n",
    "# i need to create four groups 1. self congurent (sc) 2. self incongruent(si) 3.other congruent(oc) 4.other incongruent (oi)\n",
    "sc=['10_Q175','12_Q175','15_Q175','16_Q175','20_Q175','25_Q175','26_Q175','27_Q175','28_Q175','33_Q175','34_Q175','42_Q175','47_Q175','49_Q175'\n",
    ",'61_Q175','62_Q175']\n",
    "si=['1_Q175','7_Q175','14_Q175','18_Q175','21_Q175','31_Q175','36_Q175','38_Q175','41_Q175','46_Q175','51_Q175','56_Q175','59_Q175','60_Q175'\n",
    ",'63_Q175','64_Q175']\n",
    "oc=['2_Q175','3_Q175','4_Q175','6_Q175','9_Q175','11_Q175','17_Q175','24_Q175','29_Q175','32_Q175','37_Q175','44_Q175','45_Q175','50_Q175'\n",
    ",'54_Q175','58_Q175']\n",
    "oi=['5_Q175','8_Q175','13_Q175','19_Q175','22_Q175','23_Q175','30_Q175','35_Q175','39_Q175','40_Q175','43_Q175','48_Q175','52_Q175','53_Q175'\n",
    ",'55_Q175','57_Q175']\n",
    "# remove trials 3 and 63 due to wrong link of picture for participants before the date 17.10.2022\n",
    "#append both dfs\n",
    "df=df_left.append(df_right)\n",
    "#column for sum of sc\n",
    "df['self_congurent']= df.apply(lambda row: row[sc].sum(),axis=1)\n",
    "df['self_congurent']=(16-df['self_congurent'])/16\n",
    "#column for sum of si\n",
    "df['self_incongurent']=df.apply(lambda row: row[si].sum(),axis=1)\n",
    "df['self_incongurent']=(16-df['self_incongurent'])/16\n",
    "#column for sum of oc\n",
    "df['other_congurent']=df.apply(lambda row: row[oc].sum(),axis=1)\n",
    "df['other_congurent']=(16-df['other_congurent'])/16\n",
    "#column for sum of oi\n",
    "df['other_incongurent']=df.apply(lambda row: row[oi].sum(),axis=1)\n",
    "df['other_incongurent']=(16-df['other_incongurent'])/16\n",
    "#calculate the incongurent! if value is >0 means self incongurent and if <0 other ingurent and 0 means no ingurennt\n",
    "df['perspective']=(df['self_incongurent']-df['self_congurent'])-(df['other_incongurent']-df['other_congurent'])\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "df.rename(columns={'StartDate':'Date','Q132_1':'Vp-Code'},inplace=True)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "#export only necessary cols for statistical analysis\n",
    "cols_wanted=['Date','Duration (in seconds)','Vp-Code','TICS', 'TICS_control1', 'TICS_control2', 'TICS_control3', 'SVO_Final','BIS_Total_qs', 'BAS_total_qs', 'BAS_drive', 'BAS_FUN', 'BAS_rewards', 'BIS_15_MI', 'BIS_15_NPI', 'BIS_15_ABI',\n",
    " 'BIS_15_total', 'BFI_N', 'BFI_E', 'BFI_O', 'BFI_A', 'BFI_C', 'crt_ball', 'crt_machine', 'crt_lake', 'stai_total', 'sds_total',\n",
    " 'meq_total', 'e-scale_total','perspective']\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "df=df[cols_wanted]\n",
    "#i need to change the name of ANPE07B which is for the VP HETO16B\n",
    "df=df.sort_values(by='Vp-Code')\n",
    "df.iloc[2,2]='HETO16B'\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "#export the edited DF\n",
    "df_append=pd.ExcelWriter('df_online_DOM.xlsx')\n",
    "df.to_excel(df_append)\n",
    "df_append.save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "<ipython-input-36-6e75f4705e78>:803: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_left['perspective-order']='left'\n",
      "<ipython-input-36-6e75f4705e78>:815: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_left[ql5c]= np.where(df_left[ql5c]==5, 0, 1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1736: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n",
      "<ipython-input-36-6e75f4705e78>:825: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_left[ql4c]= np.where(df_left[ql4c]==4, 0, 1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1736: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value[:, i].tolist())\n"
     ]
    }
   ],
   "source": [
    "#Online Luca\n",
    "\n",
    "\n",
    "#here I will export the output of online QS of lucas' study\n",
    "#exported data should be numeric\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#upload raw data from inline questionnaire\n",
    "raw_onlinefb=pd.read_excel(\"online_luca_raw.xlsx\")\n",
    "#drop the first unwanted row\n",
    "raw_onlinefb=raw_onlinefb.drop([0], axis=0)\n",
    "\n",
    "#drop unwanted columns\n",
    "raw_onlinefb=raw_onlinefb.drop([ 'EndDate', 'Status', 'IPAddress','Finished', \n",
    "       'RecipientLastName', 'RecordedDate', 'ResponseId', 'RecipientFirstName', 'RecipientEmail', 'ExternalReference', 'LocationLatitude',\n",
    "                                'LocationLongitude', 'UserLanguage'], axis=1)\n",
    "distribution=['anonymous','qr']\n",
    "raw_onlinefb=raw_onlinefb.loc[raw_onlinefb['DistributionChannel'].isin(distribution)]\n",
    "#drop the participants did not complete their survey\n",
    "raw_onlinefb=raw_onlinefb[raw_onlinefb['Progress']==100] \n",
    "\n",
    "\n",
    "#drop the cols of VP codes if they are empty\n",
    "raw_onlinefb.dropna(subset=['Q132_1'], inplace=True)\n",
    "\n",
    "#columns that cover TICS QS\n",
    "df_tics=['Q2_1', 'Q2_2', 'Q2_3', 'Q2_4', 'Q2_5', 'Q111_1', 'Q111_2', 'Q111_3', 'Q111_4', 'Q111_5',\n",
    "         'Q76_1', 'Q76_2','Q76_3', 'Q76_5', 'Q110_1', 'Q110_2', 'Q110_3', 'Q110_4', \n",
    "         'Q110_5', 'Q77_1', 'Q77_2', 'Q77_3', 'Q77_4', 'Q77_5', 'Q112_1',  \n",
    "         'Q112_2', 'Q112_3', 'Q112_4', 'Q112_5', 'Q78_1', 'Q78_2','Q78_3', \n",
    "         'Q78_4', 'Q78_5', 'Q113_1', 'Q113_2', 'Q113_3', 'Q113_4',\n",
    "         'Q113_5', 'Q79_1', 'Q79_2', 'Q79_3', 'Q79_4', 'Q114_1','Q114_2', \n",
    "         'Q114_3', 'Q114_4', 'Q114_5', 'Q80_1',  'Q80_3',\n",
    "         'Q80_4', 'Q80_5', 'Q115_1', 'Q115_2', 'Q115_3', 'Q115_4',\n",
    "         'Q115_5']\n",
    "\n",
    "#There are three control question that \n",
    "#every VP need to have three of that in his/her DF\n",
    "control_qs=['Q76_4', 'Q79_5','Q80_2',]\n",
    "Control_1=[]\n",
    "Control_2=[]\n",
    "Control_3=[]\n",
    "for i in raw_onlinefb[control_qs[0]]:\n",
    "    if i==4:\n",
    "        a='True'\n",
    "        Control_1.append(a)\n",
    "    else:\n",
    "        \n",
    "        b='false'\n",
    "        Control_1.append(b)\n",
    "    \n",
    "for i in raw_onlinefb[control_qs[1]]:\n",
    "    if i==1:\n",
    "        a='True'\n",
    "        Control_2.append(a)\n",
    "    else:\n",
    "        \n",
    "        b='False'\n",
    "        Control_2.append(b)   \n",
    "for i in raw_onlinefb[control_qs[2]]:\n",
    "    if i==2:\n",
    "        a='True'\n",
    "        Control_3.append(a)\n",
    "    else:\n",
    "        \n",
    "        b='False'\n",
    "        Control_3.append(b)    \n",
    "\n",
    "#identifying the 9-subgroups of TICS\n",
    "#category and its questions\n",
    "#items covering work overload\n",
    "WOL=[1,4,44,54,17,27,38,50]\n",
    "#items covering social overload\n",
    "SO=[49,57,7,19,28,39]\n",
    "#items covering Pressure to Perform\n",
    "PP=[8,12,14,22,30,23,32,40,43]\n",
    "#items covering work discontent\n",
    "WD=[5,10,13,41,21,37,48,53]\n",
    "#items covering Excessive Demands at Work\n",
    "EDW=[3,47,20,24,35,55]\n",
    "#items covering Lack of Social Recognition\n",
    "LSR=[2,18,31,46]\n",
    "#items covering Social Tensions\n",
    "ST=[6,33,15,26,45,52]\n",
    "#items covering social isolation\n",
    "SI=[29,34,11,42,51,56]\n",
    "#chronic worring\n",
    "CW=[9,16,25,36]\n",
    "tics_num=[]\n",
    "for i in range(1,61):\n",
    "    n=i\n",
    "    if n in LSR:\n",
    "        n=str(n)+\"_\"+'LSR'\n",
    "        tics_num.append(n)\n",
    "    elif n in SO:\n",
    "        n=str(n)+\"_\"+'SO'\n",
    "        tics_num.append(n)\n",
    "    elif n in PP:\n",
    "        n=str(n)+\"_\"+'PP'\n",
    "        tics_num.append(n)\n",
    "    elif n in WD:\n",
    "        n=str(n)+\"_\"+'WD'\n",
    "        tics_num.append(n)\n",
    "    elif n in EDW:\n",
    "        n=str(n)+\"_\"+'EDW'\n",
    "        tics_num.append(n)\n",
    "    elif n in WOL:\n",
    "        n=str(n)+\"_\"+'WOL'\n",
    "        tics_num.append(n)\n",
    "    elif n in ST:\n",
    "        n=str(n)+\"_\"+'ST'\n",
    "        tics_num.append(n)\n",
    "    elif n in SI:\n",
    "        n=str(n)+\"_\"+'SI'\n",
    "        tics_num.append(n)\n",
    "    elif n in CW:\n",
    "        n=str(n)+\"_\"+'CW'\n",
    "        tics_num.append(n)\n",
    "        \n",
    "#Convert label of tic cols into numbers\n",
    "#with shoeing they belong to which subcategory of tics\n",
    "tics_dic=dict(zip(df_tics,tics_num))   \n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q2_1': '1_WOL', 'Q2_2': '2_LSR' ,'Q2_3': '3_EDW', 'Q2_4': '4_WOL',\n",
    "                                          'Q2_5': '5_WD',\n",
    " 'Q111_1': '6_ST', 'Q111_2': '7_SO', 'Q111_3': '8_PP', 'Q111_4': '9_CW', 'Q111_5': '10_WD', 'Q76_1': '11_SI', 'Q76_2': '12_PP', 'Q76_3': '13_WD',\n",
    " 'Q76_5': '14_PP',\n",
    " 'Q110_1': '15_ST', 'Q110_2': '16_CW', 'Q110_3': '17_WOL', 'Q110_4': '18_LSR', 'Q110_5': '19_SO', 'Q77_1': '20_EDW', 'Q77_2': '21_WD', 'Q77_3': '22_PP', 'Q77_4': '23_PP', 'Q77_5': '24_EDW', 'Q112_1': '25_CW', 'Q112_2': '26_ST', 'Q112_3': '27_WOL', 'Q112_4': '28_SO', 'Q112_5': '29_SI', 'Q78_1': '30_PP',\n",
    " 'Q78_2': '31_LSR',\n",
    " 'Q78_3': '32_PP',\n",
    " 'Q78_4': '33_ST', 'Q78_5': '34_SI', 'Q113_1': '35_EDW', 'Q113_2': '36_CW', 'Q113_3': '37_WD', 'Q113_4': '38_WOL', 'Q113_5': '39_SO', 'Q79_1': '40_PP', 'Q79_2': '41_WD',\n",
    " 'Q79_3': '42_SI', 'Q79_4': '43_PP', 'Q114_1': '44_WOL', 'Q114_2': '45_ST', 'Q114_3': '46_LSR', 'Q114_4': '47_EDW', 'Q114_5': '48_WD',\n",
    " 'Q80_1': '49_SO', 'Q80_3': '50_WOL', 'Q80_4': '51_SI', 'Q80_5': '52_ST',\n",
    " 'Q115_1': '53_WD', 'Q115_2': '54_WOL', 'Q115_3': '55_EDW', 'Q115_4': '56_SI', 'Q115_5': '57_SO'})\n",
    "raw_onlinefb['TICS']=raw_onlinefb[tics_num].sum(axis=1)\n",
    "raw_onlinefb['TICS_control1']=Control_1\n",
    "raw_onlinefb['TICS_control2']=Control_2\n",
    "raw_onlinefb['TICS_control3']=Control_3\n",
    "\n",
    "\n",
    "#SVO data preparation\n",
    "#to calculate the social value orientation\n",
    "svo=['Q92_1','Q103_1','Q102_1','Q101_1','Q100_1','Q99_1','Q98_1','Q97_1','Q96_1',]\n",
    "svo_num=['svo_1','svo_2','svo_3','svo_4','svo_5','svo_6','svo_7','svo_8','svo_9']\n",
    "svo_dict=dict(zip(svo,svo_num))\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q92_1': 'svo_1',\n",
    " 'Q103_1': 'svo_2',\n",
    " 'Q102_1': 'svo_3',\n",
    " 'Q101_1': 'svo_4',\n",
    " 'Q100_1': 'svo_5',\n",
    " 'Q99_1': 'svo_6',\n",
    " 'Q98_1': 'svo_7',\n",
    " 'Q97_1': 'svo_8',\n",
    " 'Q96_1': 'svo_9'})\n",
    "#individualistic is H\n",
    "#Competitive Com\n",
    "# properative pro\n",
    "#the order of orientations in online qs of SVO\n",
    "#I-Com-pro\tCom-pro-I\tpro-I-Com\tI-Com-pro\tCom-pro-I\n",
    "#pro-I-Com\tpro-Com-I\tCom-I-pro\tI-pro-Com\n",
    "#0 means pro/1 means Com/ 2 means I\n",
    "#I-Com-pro\n",
    "#the empty rows are called NONE\n",
    "svo_1=[]\n",
    "for i in raw_onlinefb['svo_1']:\n",
    "    if i==1:\n",
    "        svo_q1='ind'\n",
    "        svo_1.append(svo_q1)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q1='Com'\n",
    "        svo_1.append(svo_q1)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q1='pro'\n",
    "        svo_1.append(svo_q1)  \n",
    "    else:\n",
    "        svo_q1='None'\n",
    "        svo_1.append(svo_q1)\n",
    "        \n",
    "#add the svo_1 col\n",
    "raw_onlinefb['svo_1']=svo_1\n",
    "#Com-pro-I       \n",
    "#svo_q2\n",
    "svo_2=[]\n",
    "for i in raw_onlinefb['svo_2']:\n",
    "    if i==1:\n",
    "        svo_q2='Com'\n",
    "        svo_2.append(svo_q2)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q2='pro'\n",
    "        svo_2.append(svo_q2)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q2='ind'\n",
    "        svo_2.append(svo_q2) \n",
    "    else:\n",
    "        svo_q2='None'\n",
    "        svo_2.append(svo_q2)\n",
    "        \n",
    "    \n",
    "        \n",
    "#add the svo_2 col\n",
    "raw_onlinefb['svo_2']=svo_2\n",
    "#pro-I-Com\n",
    "#svo_q3\n",
    "svo_3=[]\n",
    "for i in raw_onlinefb['svo_3']:\n",
    "    if i==1:\n",
    "        svo_q3='pro'\n",
    "        svo_3.append(svo_q3)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q3='ind'\n",
    "        svo_3.append(svo_q3)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q3='Com'\n",
    "        svo_3.append(svo_q3)\n",
    "    else:\n",
    "        svo_q3='None'\n",
    "        svo_3.append(svo_q3)\n",
    "        \n",
    "#add the svo_3 col\n",
    "raw_onlinefb['svo_3']=svo_3\n",
    "\n",
    "#I-Com-pro\n",
    "svo_4=[]\n",
    "for i in raw_onlinefb['svo_4']:\n",
    "    if i==1:\n",
    "        svo_q4='ind'\n",
    "        svo_4.append(svo_q4)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q4='Com'\n",
    "        svo_4.append(svo_q4)\n",
    "    \n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q4='pro'\n",
    "        svo_4.append(svo_q4) \n",
    "    else:\n",
    "        svo_q4='None'\n",
    "        svo_4.append(svo_q4)\n",
    "        \n",
    "#add the svo_4 col\n",
    "raw_onlinefb['svo_4']=svo_4  \n",
    "\n",
    "\n",
    "#Com-pro-I       \n",
    "#svo_q5\n",
    "svo_5=[]\n",
    "for i in raw_onlinefb['svo_5']:\n",
    "    if i==1:\n",
    "        svo_q5='Com'\n",
    "        svo_5.append(svo_q5)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q5='pro'\n",
    "        svo_5.append(svo_q5)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q5='ind'\n",
    "        svo_5.append(svo_q5) \n",
    "    else:\n",
    "        svo_q5='None'\n",
    "        svo_5.append(svo_q5)\n",
    "        \n",
    "#add the svo_5 col\n",
    "raw_onlinefb['svo_5']=svo_5\n",
    "\n",
    "\n",
    "#pro-I-Com\n",
    "#svo_q6\n",
    "svo_6=[]\n",
    "for i in raw_onlinefb['svo_6']:\n",
    "    if i==1:\n",
    "        svo_q6='pro'\n",
    "        svo_6.append(svo_q6)\n",
    "    elif i==2:\n",
    "        \n",
    "        svo_q6='ind'\n",
    "        svo_6.append(svo_q6)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q6='Com'\n",
    "        svo_6.append(svo_q6)\n",
    "    else:\n",
    "        svo_q6='None'\n",
    "        svo_6.append(svo_q6)\n",
    "        \n",
    "#add the svo_6 col\n",
    "raw_onlinefb['svo_6']=svo_6\n",
    "\n",
    "#pro-Com-I\n",
    "#svo_7\n",
    "svo_7=[]\n",
    "for i in raw_onlinefb['svo_7']:\n",
    "    if i==1:\n",
    "        svo_q7='pro'\n",
    "        svo_7.append(svo_q7)\n",
    "    elif i==2:\n",
    "        svo_q7='Com'\n",
    "        svo_7.append(svo_q7)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q7='ind'\n",
    "        svo_7.append(svo_q7)\n",
    "    else:\n",
    "        svo_q7='None'\n",
    "        svo_7.append(svo_q7)\n",
    "        \n",
    "#add the svo_7 col\n",
    "raw_onlinefb['svo_7']=svo_7\n",
    "\n",
    "#Com-I-pro\n",
    "#svo_8\n",
    "svo_8=[]\n",
    "for i in raw_onlinefb['svo_8']:\n",
    "    if i==1:\n",
    "        svo_q8='Com'\n",
    "        svo_8.append(svo_q8)\n",
    "    elif i==2:\n",
    "        svo_q8='ind'\n",
    "        svo_8.append(svo_q8)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q8='pro'\n",
    "        svo_8.append(svo_q8)\n",
    "    else:\n",
    "        svo_q8='None'\n",
    "        svo_8.append(svo_q8)\n",
    "#add the svo_8 col\n",
    "raw_onlinefb['svo_8']=svo_8\n",
    "\n",
    "#I-pro-Com\n",
    "#svo_9\n",
    "svo_9=[]\n",
    "for i in raw_onlinefb['svo_9']:\n",
    "    if i==1:\n",
    "        svo_q9='ind'\n",
    "        svo_9.append(svo_q9)\n",
    "    elif i==2:\n",
    "        svo_q9='pro'\n",
    "        svo_9.append(svo_q9)\n",
    "    elif i==3:\n",
    "        \n",
    "        svo_q9='Com'\n",
    "        svo_9.append(svo_q9)\n",
    "    else:\n",
    "        svo_q9='None'\n",
    "        svo_9.append(svo_q9)\n",
    "\n",
    "#add the svo_9 col\n",
    "raw_onlinefb['svo_9']=svo_9\n",
    "#i need to convert the number of times a word is there\n",
    "pro = ['pro']\n",
    "ind = ['ind']\n",
    "com = ['Com']\n",
    "raw_onlinefb[\"prosocial\"] = raw_onlinefb.isin(pro).sum(axis='columns')\n",
    "raw_onlinefb[\"individualistic\"] = raw_onlinefb.isin(ind).sum(axis='columns')\n",
    "raw_onlinefb[\"competetive\"] = raw_onlinefb.isin(com).sum(axis='columns')\n",
    "#create a column to clarify fainall y hwre every one should be in the category \n",
    "# if someone has 6 or more times made a decision you choose that category of SVO for the person\n",
    "conditions = [\n",
    "    (raw_onlinefb[\"prosocial\"] >= 6),\n",
    "    (raw_onlinefb[\"individualistic\"] >=6) , (raw_onlinefb[\"competetive\"] >= 6)]\n",
    "values = [\"prosocial\", \"individualistic\", \"competetive\"]\n",
    "raw_onlinefb['SVO_Final'] = np.select(conditions, values)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#to prepare data for analysis of BIS/BAS\n",
    "#Q75_1\tQ75_2\tQ75_3\tQ75_4\t\n",
    "#Q75_5\tQ75_6\tQ116_1\tQ116_2\tQ116_3\tQ116_4\tQ116_5\tQ116_6\n",
    "#Q117_1\tQ117_2\tQ117_3\tQ117_4\tQ117_5\n",
    "#Q117_6\tQ118_1\tQ118_2\tQ118_3\tQ118_4\tQ118_5\tQ118_6\n",
    "#raw_onlinefb.columns.to_list()\n",
    "\n",
    "#Items 1, 3-21, 23-24 are reverse-scored.\n",
    "#Items 2 and 22 are scored normally.\n",
    " \n",
    "#4 Subscales: Behavioral Inhibition System (BIS), Behavioral Activation System (BAS) Drive, BAS Fun Seeking, BAS Reward Responsiveness\n",
    "#BIS:  items 2, 8, 13, 16, 19, 22, 24\n",
    "#BAS Drive:  items 3, 9, 12, 21 \n",
    "#BAS Fun Seeking:  items 5, 10, 15, 20 \n",
    "#BAS Reward Responsiveness:  items 4, 7, 14, 18, 23\n",
    "bis_bas=['Q75_1','Q75_2','Q75_3','Q75_4','Q75_5','Q75_6','Q116_1',\n",
    "         'Q116_2','Q116_3',\n",
    "         'Q116_4','Q116_5','Q116_6',\n",
    "         'Q117_1','Q117_2','Q117_3','Q117_4',\n",
    "         'Q117_5','Q117_6','Q118_1','Q118_2','Q118_3','Q118_4','Q118_5',\n",
    "         'Q118_6']\n",
    "BIS_Total=[2, 8, 13, 16, 19, 22, 24]\n",
    "BAS_total=[3, 4, 5, 7, 9, 10, 12, 14, 15, 18, 20, 21, 23]\n",
    "BAS_drive=[3, 9, 12, 21 ]\n",
    "BAS_FUN=[5, 10, 15, 20]\n",
    "BAS_rewards=[4, 7, 14, 18, 23]\n",
    "Filler=[1,6,11,17]\n",
    "BIS_BAS_num=[]\n",
    "#to find qs excist in all and also to create a groupe separately\n",
    "BAS_group=[BAS_total,BAS_FUN,BAS_rewards]\n",
    "\n",
    "for i in range(1,25):\n",
    "\n",
    "    if i in (BAS_total and BAS_FUN):\n",
    "        i=str(i)+\"_\"+'total_fun'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in (BAS_total and BAS_rewards):\n",
    "        i=str(i)+\"_\"+'total_reward'\n",
    "        BIS_BAS_num.append(i)    \n",
    "\n",
    "    elif i in BAS_drive:\n",
    "        i=str(i)+\"_\"+'BAS_drive'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in BAS_FUN:\n",
    "        i=str(i)+\"_\"+'BAS_FUN'\n",
    "        BIS_BAS_num.append(i)    \n",
    "    elif i in BAS_rewards:\n",
    "        i=str(i)+\"_\"+'BAS_rewards'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in BIS_Total:\n",
    "        i=str(i)+\"_\"+'BIS_Total'\n",
    "        BIS_BAS_num.append(i)\n",
    "    elif i in Filler:\n",
    "        i=str(i)+\"_\"+'Filler'\n",
    "        BIS_BAS_num.append(i)      \n",
    "\n",
    "#make a dic to re_label the raw col names from raw data for BIS/BAS test\n",
    "#bisbas_dic=dict(zip(bis_bas,BIS_BAS_num))   \n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q75_1': '1_Filler','Q75_2': '2_BIS_Total','Q75_3': '3_BAS_drive',\n",
    "                                          'Q75_4': '4_total_reward','Q75_5': '5_total_fun','Q75_6': '6_Filler',\n",
    " 'Q116_1': '7_total_reward', 'Q116_2': '8_BIS_Total', 'Q116_3': '9_BAS_drive', 'Q116_4': '10_total_fun', 'Q116_5': '11_Filler', 'Q116_6': '12_BAS_drive', 'Q117_1': '13_BIS_Total', 'Q117_2': '14_total_reward',\n",
    " 'Q117_3': '15_total_fun', 'Q117_4': '16_BIS_Total', 'Q117_5': '17_Filler', 'Q117_6': '18_total_reward', 'Q118_1': '19_BIS_Total',\n",
    " 'Q118_2': '20_total_fun', 'Q118_3': '21_BAS_drive', 'Q118_4': '22_BIS_Total', 'Q118_5': '23_total_reward', 'Q118_6': '24_BIS_Total'})\n",
    "\n",
    "#there are some reverse scoring\n",
    "raw_onlinefb['2_BIS_Total'] = raw_onlinefb['2_BIS_Total'].map({1:4, 2:3, 3:2, 4:1})\n",
    "raw_onlinefb['22_BIS_Total'] = raw_onlinefb['22_BIS_Total'].map({1:4, 2:3, 3:2, 4:1})    \n",
    "\n",
    "#adding columns based on total BIS or BAs and also sub-factors\n",
    "BIS_Total_qs=['2_BIS_Total','8_BIS_Total','13_BIS_Total','16_BIS_Total','19_BIS_Total','22_BIS_Total','24_BIS_Total']\n",
    "raw_onlinefb['BIS_Total_qs']=raw_onlinefb[BIS_Total_qs].sum(axis=1)\n",
    "\n",
    "BAS_total_qs=['3_BAS_drive','4_total_reward','5_total_fun','7_total_reward','9_BAS_drive','10_total_fun',\n",
    "           '12_BAS_drive','14_total_reward','15_total_fun','18_total_reward','20_total_fun','21_BAS_drive','23_total_reward']\n",
    "raw_onlinefb['BAS_total_qs']=raw_onlinefb[BAS_total_qs].sum(axis=1)\n",
    "\n",
    "BAS_drive=['3_BAS_drive','9_BAS_drive','12_BAS_drive','21_BAS_drive']\n",
    "raw_onlinefb['BAS_drive']=raw_onlinefb[BAS_drive].sum(axis=1)\n",
    "\n",
    "BAS_FUN=['5_total_fun','10_total_fun','15_total_fun','20_total_fun']\n",
    "raw_onlinefb['BAS_FUN']=raw_onlinefb[BAS_FUN].sum(axis=1)\n",
    "\n",
    "BAS_rewards=['4_total_reward','7_total_reward','14_total_reward','18_total_reward','23_total_reward']\n",
    "raw_onlinefb['BAS_rewards']=raw_onlinefb[BAS_rewards].sum(axis=1)\n",
    "\n",
    "Filler=['1_Filler','6_Filler','11_Filler','17_Filler']\n",
    "raw_onlinefb['Filler']=raw_onlinefb[Filler].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#to prepare BIS_15 qs for data analysis\n",
    "#cols that include the bis 15 qs\n",
    "Bis_15_raw=['Q82_1','Q82_2','Q82_3','Q82_4','Q82_5','Q85_1','Q85_2','Q85_3','Q85_4','Q85_5','Q86_1',\n",
    "'Q86_2','Q86_3','Q86_4','Q86_5']\n",
    "#non-planning impulsiveness\n",
    "NPI=[1,5,7,8,15]\n",
    "#motor impulsivity\n",
    "MI=[2,10,12,13,9]\n",
    "#Attention-based impulsivity.\n",
    "ABI=[14,6,4,3,11]\n",
    "#make a list of BIS qs\n",
    "BIS_15_lis=[]\n",
    "for i in range(1,16):\n",
    "    if i in NPI:\n",
    "        i=str(i)+\"-\"+'BIS-15-NPI'\n",
    "        BIS_15_lis.append(i)\n",
    "    if i in MI:\n",
    "        i=str(i)+\"-\"+'BIS-15-MI'\n",
    "        BIS_15_lis.append(i)\n",
    "    if i in ABI:\n",
    "        i=str(i)+\"-\"+'BIS-15-ABI'\n",
    "        BIS_15_lis.append(i)\n",
    "\n",
    "#need to synch raw with edited col names\n",
    "#bis15_dic=dict(zip(Bis_15_raw,BIS_15_lis))  \n",
    "#rename the bis15 cols\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q82_1': '1-BIS-15-NPI',\n",
    " 'Q82_2': '2-BIS-15-MI',\n",
    " 'Q82_3': '3-BIS-15-ABI', 'Q82_4': '4-BIS-15-ABI', 'Q82_5': '5-BIS-15-NPI', 'Q85_1': '6-BIS-15-ABI', 'Q85_2': '7-BIS-15-NPI', 'Q85_3': '8-BIS-15-NPI','Q85_4': '9-BIS-15-MI',\n",
    "'Q85_5': '10-BIS-15-MI', 'Q86_1': '11-BIS-15-ABI',\n",
    " 'Q86_2': '12-BIS-15-MI', 'Q86_3': '13-BIS-15-MI', 'Q86_4': '14-BIS-15-ABI', 'Q86_5': '15-BIS-15-NPI'})\n",
    "\n",
    "#there are qs need to be reversed for scoring\n",
    "#questions should be reversed 13,15,1,7,4\n",
    "raw_onlinefb['13-BIS-15-MI'] = raw_onlinefb['13-BIS-15-MI'].map({1:4, 2:3, 3:2, 4:1})\n",
    "raw_onlinefb['15-BIS-15-NPI']= raw_onlinefb['15-BIS-15-NPI'].map({1:4, 2:3, 3:2, 4:1})    \n",
    "raw_onlinefb['1-BIS-15-NPI'] = raw_onlinefb['1-BIS-15-NPI'].map({1:4, 2:3, 3:2, 4:1})\n",
    "raw_onlinefb['7-BIS-15-NPI'] = raw_onlinefb['7-BIS-15-NPI'].map({1:4, 2:3, 3:2, 4:1})   \n",
    "raw_onlinefb['4-BIS-15-ABI'] = raw_onlinefb['4-BIS-15-ABI'].map({1:4, 2:3, 3:2, 4:1})\n",
    "\n",
    "#make new cols for each subcategory of BIS15 and also a total one\n",
    "bis_15_mi=['2-BIS-15-MI','9-BIS-15-MI','10-BIS-15-MI','12-BIS-15-MI','13-BIS-15-MI']\n",
    "raw_onlinefb['BIS_15_MI']=raw_onlinefb[bis_15_mi].sum(axis=1)\n",
    "\n",
    "bis_15_npi=['1-BIS-15-NPI','5-BIS-15-NPI','7-BIS-15-NPI','8-BIS-15-NPI','15-BIS-15-NPI']\n",
    "raw_onlinefb['BIS_15_NPI']=raw_onlinefb[bis_15_npi].sum(axis=1)\n",
    "\n",
    "bis_15_abi=['3-BIS-15-ABI','4-BIS-15-ABI','6-BIS-15-ABI','11-BIS-15-ABI','14-BIS-15-ABI']\n",
    "raw_onlinefb['BIS_15_ABI']=raw_onlinefb[bis_15_abi].sum(axis=1)\n",
    "\n",
    "#create a col form all 15 items of BiS-15\n",
    "BIS_15_total=['2-BIS-15-MI','9-BIS-15-MI','10-BIS-15-MI','12-BIS-15-MI','13-BIS-15-MI',\n",
    "       '1-BIS-15-NPI','5-BIS-15-NPI','7-BIS-15-NPI','8-BIS-15-NPI','15-BIS-15-NPI',\n",
    "       '3-BIS-15-ABI','4-BIS-15-ABI','6-BIS-15-ABI','11-BIS-15-ABI','14-BIS-15-ABI']\n",
    "raw_onlinefb['BIS_15_total']=raw_onlinefb[BIS_15_total].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#BFI data prepration\n",
    "BFI_Raw=['Q135_1', 'Q135_2', 'Q135_3', 'Q135_4', 'Q135_5', 'Q136_1',\n",
    "         'Q136_2','Q136_3','Q136_4','Q136_5']\n",
    "#five dimension of BFI\n",
    "N=[4,9]\n",
    "E=[1,6]\n",
    "O=[5,10]\n",
    "A=[2,7]\n",
    "C=[3,8]\n",
    "#list of BFI\n",
    "BFI_list=[]\n",
    "for i in range(1,11):\n",
    "    if i in N:\n",
    "        i=str(i)+\"-\"+'BFI_N'\n",
    "        BFI_list.append(i)\n",
    "    if i in E:\n",
    "        i=str(i)+\"-\"+'BFI_E'\n",
    "        BFI_list.append(i)\n",
    "    if i in O:\n",
    "        i=str(i)+\"-\"+'BFI_O'\n",
    "        BFI_list.append(i)        \n",
    "    if i in A:\n",
    "        i=str(i)+\"-\"+'BFI_A'\n",
    "        BFI_list.append(i)\n",
    "    if i in C:\n",
    "        i=str(i)+\"-\"+'BFI_C'\n",
    "        BFI_list.append(i)\n",
    "\n",
    "#need to synch raw with edited col names\n",
    "BFI_dic=dict(zip(BFI_Raw,BFI_list))  \n",
    "\n",
    "#rename the bis15 cols\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q135_1': '1-BFI_E', 'Q135_2': '2-BFI_A','Q135_3': '3-BFI_C',                                           'Q135_4': '4-BFI_N',\n",
    "'Q135_5': '5-BFI_O','Q136_1': '6-BFI_E', 'Q136_2': '7-BFI_A', 'Q136_3': '8-BFI_C','Q136_4': '9-BFI_N', 'Q136_5': '10-BFI_O'})\n",
    "\n",
    "#BFI- questions should be reversed 1,3,4,5,7\n",
    "raw_onlinefb['1-BFI_E'] = raw_onlinefb['1-BFI_E'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['3-BFI_C'] = raw_onlinefb['3-BFI_C'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['4-BFI_N'] = raw_onlinefb['4-BFI_N'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['5-BFI_O'] = raw_onlinefb['5-BFI_O'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['7-BFI_A'] = raw_onlinefb['7-BFI_A'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "#to calculate mean of each dimension\n",
    "# N=[4,9]\n",
    "# E=[1,6]\n",
    "# O=[5,10]\n",
    "# A=[2,7]\n",
    "# C=[3,8]\n",
    "#create new cols for calclualtion of each dim of BFI (mean)\n",
    "bfi_n=['4-BFI_N','9-BFI_N']\n",
    "raw_onlinefb['BFI_N']=raw_onlinefb[bfi_n].mean(axis=1)\n",
    "bfi_e=['1-BFI_E','6-BFI_E']\n",
    "raw_onlinefb['BFI_E']=raw_onlinefb[bfi_e].mean(axis=1)\n",
    "bfi_o=['5-BFI_O','10-BFI_O']\n",
    "raw_onlinefb['BFI_O']=raw_onlinefb[bfi_o].mean(axis=1)\n",
    "bfi_a=['2-BFI_A','7-BFI_A']\n",
    "raw_onlinefb['BFI_A']=raw_onlinefb[bfi_a].mean(axis=1)\n",
    "bfi_c=['3-BFI_C','8-BFI_C']\n",
    "raw_onlinefb['BFI_C']=raw_onlinefb[bfi_c].mean(axis=1)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "#CRT-there are three questions with right answer \n",
    "crt_ball=5\n",
    "crt_machine=5\n",
    "crt_lake=47\n",
    "#raw col names\n",
    "crt_raw=['Q88_1','Q89_1','Q90_1']\n",
    "#rename the raw cols\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q88_1':'CRT_ball','Q89_1':'CRT_machine','Q90_1':'CRT_lake'})\n",
    "#create empty list for three crts\n",
    "crt_ball=[]\n",
    "crt_machine=[]\n",
    "crt_lake=[]\n",
    "#there are different kind of answers for each crt in raw data\n",
    "crt_ball_answers=['5','0,05','5 cent']\n",
    "crt_machine_answers=['5','5 Minuten']\n",
    "crt_lake_answers=['47','47 Tage']\n",
    "for i in raw_onlinefb.CRT_ball:\n",
    "    if i in crt_ball_answers:\n",
    "        i='True'\n",
    "        crt_ball.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_ball.append(i)\n",
    "for i in raw_onlinefb.CRT_machine:\n",
    "    if i in crt_machine_answers:\n",
    "        i='True'\n",
    "        crt_machine.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_machine.append(i)\n",
    "for i in raw_onlinefb.CRT_machine:\n",
    "    if i in crt_machine_answers:\n",
    "        i='True'\n",
    "        crt_machine.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_machine.append(i)\n",
    "for i in raw_onlinefb.CRT_lake:\n",
    "    if i in crt_lake_answers:\n",
    "        i='True'\n",
    "        crt_lake.append(i)\n",
    "    else:\n",
    "        i='False'\n",
    "        crt_lake.append(i)\n",
    "#creating new graphs with results of for loop for each crt question\n",
    "raw_onlinefb['crt_ball']=crt_ball\n",
    "raw_onlinefb['crt_machine']=crt_lake\n",
    "raw_onlinefb['crt_lake']=crt_lake\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#to prepare the STAI qs for data analysis\n",
    "#For STAI, there are two set of qs, each 20.\n",
    "\n",
    "#the name of stai cols in raw data\n",
    "stais_raw=['Q67_1', 'Q67_2', 'Q67_3', 'Q67_4', 'Q67_5', 'Q119_1', 'Q119_2',\n",
    " 'Q119_3', 'Q119_4', 'Q119_5', 'Q120_1', 'Q120_2', 'Q120_3', 'Q120_4', 'Q120_5',\n",
    " 'Q121_1', 'Q121_2', 'Q121_3', 'Q121_4', 'Q121_5', 'Q71_1', 'Q71_2', 'Q71_3',\n",
    " 'Q71_4', 'Q71_5', 'Q122_1', 'Q122_2', 'Q122_3', 'Q122_4', 'Q122_5', 'Q123_1', 'Q123_2',\n",
    " 'Q123_3', 'Q123_4', 'Q123_5', 'Q124_1', 'Q124_2', 'Q124_3', 'Q124_4', 'Q124_5']\n",
    "nums=[]\n",
    "for i in range(1,41):\n",
    "    i=str(i)+\"-\"+'stai'\n",
    "    nums.append(i)\n",
    "    \n",
    "#need to synch raw with nums\n",
    "stai_dic=dict(zip(stais_raw,nums)) \n",
    "#rename the cols from raw to created ones\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q67_1': '1-stai', 'Q67_2': '2-stai', 'Q67_3': '3-stai', 'Q67_4': '4-stai', 'Q67_5': '5-stai',\n",
    " 'Q119_1': '6-stai', 'Q119_2': '7-stai', 'Q119_3': '8-stai', 'Q119_4': '9-stai', 'Q119_5': '10-stai', 'Q120_1': '11-stai', 'Q120_2': '12-stai',\n",
    " 'Q120_3': '13-stai', 'Q120_4': '14-stai', 'Q120_5': '15-stai', 'Q121_1': '16-stai', 'Q121_2': '17-stai', 'Q121_3': '18-stai',\n",
    " 'Q121_4': '19-stai', 'Q121_5': '20-stai', 'Q71_1': '21-stai', 'Q71_2': '22-stai', 'Q71_3': '23-stai', 'Q71_4': '24-stai',\n",
    "                        'Q71_5': '25-stai', 'Q122_1': '26-stai','Q122_2': '27-stai', 'Q122_3': '28-stai', 'Q122_4': '29-stai', 'Q122_5': '30-stai',\n",
    " 'Q123_1': '31-stai', 'Q123_2': '32-stai', 'Q123_3': '33-stai', 'Q123_4': '34-stai', 'Q123_5': '35-stai', 'Q124_1': '36-stai',\n",
    " 'Q124_2': '37-stai', 'Q124_3': '38-stai', 'Q124_4': '39-stai', 'Q124_5': '40-stai'})\n",
    "#to reverse the specific questions\n",
    "##There are many questions reverted: Y1=[1,2,5,8,10,11,15,16,19,20] for Y2=[21,23,26,27,30,33,34,36,39]\n",
    "reversed_qs=['1-stai', '2-stai', '5-stai', '8-stai', '10-stai', '11-stai', '15-stai',\n",
    " '16-stai', '19-stai', '20-stai', '21-stai', '23-stai', '26-stai', '27-stai', '30-stai',\n",
    "             '33-stai', '34-stai','36-stai','39-stai']\n",
    "#reverse the answers\n",
    "raw_onlinefb[['1-stai', '2-stai', '5-stai', '8-stai', '10-stai', '11-stai', '15-stai',\n",
    " '16-stai', '19-stai', '20-stai', '21-stai', '23-stai', '26-stai', '27-stai', '30-stai',\n",
    "             '33-stai', '34-stai','36-stai','39-stai']] =raw_onlinefb[['1-stai', '2-stai', '5-stai', '8-stai', '10-stai', '11-stai', '15-stai',\n",
    " '16-stai', '19-stai', '20-stai', '21-stai', '23-stai', '26-stai', '27-stai', '30-stai',\n",
    "             '33-stai', '34-stai','36-stai','39-stai']].replace({1:4, 2:3, 3:2, 4:1})\n",
    "#calculate score of each participant for STAI\n",
    "raw_onlinefb['stai_total']=raw_onlinefb[reversed_qs].sum(axis=1)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#to prepare raw data from SDS 17\n",
    "raw_sds=['Q33', 'Q65', 'Q49', 'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60', 'Q61', 'Q62', 'Q63', 'Q64']\n",
    "# new col names\n",
    "sds_num=[]\n",
    "for i in range(1,18):\n",
    "    i=str(i)+\"-\"+'SDS17'\n",
    "    sds_num.append(i)\n",
    "#need to synch raw with nums\n",
    "sds17_dic=dict(zip(raw_sds,sds_num)) \n",
    "#rename the cols from raw to created ones\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q33': '1-SDS17', 'Q65': '2-SDS17', 'Q49': '3-SDS17', 'Q51': '4-SDS17',\n",
    "                                          'Q52': '5-SDS17', 'Q53': '6-SDS17', 'Q54': '7-SDS17', 'Q55': '8-SDS17', \n",
    "                                          'Q56': '9-SDS17', 'Q57': '10-SDS17',\n",
    " 'Q58': '11-SDS17', 'Q59': '12-SDS17', 'Q60': '13-SDS17','Q61': '14-SDS17', \n",
    " 'Q62': '15-SDS17', 'Q63': '16-SDS17', 'Q64': '17-SDS17'})\n",
    "#there is no reverse scoring in SDS17 but i should only have the sum of 1 in raw data which means True\n",
    "sds_cols=['1-SDS17', '2-SDS17', '3-SDS17', '4-SDS17', '5-SDS17', '6-SDS17', '7-SDS17', '8-SDS17', '9-SDS17', '10-SDS17',\n",
    " '11-SDS17', '12-SDS17', '13-SDS17', '14-SDS17', '15-SDS17', '16-SDS17','17-SDS17']\n",
    "\n",
    "raw_onlinefb['sds_total']=raw_onlinefb[raw_onlinefb[sds_cols]==1].sum(axis=1)         \n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#to prepare raw data of MEQ\n",
    "# MEQ scoring:\n",
    "# Q1->1-6\n",
    "# Q2->4-1\n",
    "# Q3->1-5\n",
    "# Q4->1-5\n",
    "# Q5->1-4\n",
    "meq_ls=['Q12', 'Q13', 'Q14', 'Q15', 'Q16']\n",
    "#change the name of cols\n",
    "meq_cols=['meq-1','meq-2','meq-3','meq-4','meq-5']\n",
    "#need to synch raw with nums\n",
    "meq_dict=dict(zip(meq_ls,meq_cols))\n",
    "#rename the cols from raw to created ones\n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q12': 'meq-1', 'Q13': 'meq-2', 'Q14': 'meq-3', 'Q15': 'meq-4','Q16':'meq-5'})\n",
    "#teh second q should be reversed\n",
    "raw_onlinefb['meq-2'] = raw_onlinefb['meq-2'].map({4:1, 3:2, 2:3, 1:4})\n",
    "raw_onlinefb['meq_total']=raw_onlinefb[['meq-1','meq-2','meq-3','meq-4','meq-5']].sum(axis=1)\n",
    "#the higher the number more ecening person\n",
    "\n",
    "#to prepare escale\n",
    "raw_cols=['Q138_1', 'Q138_2', 'Q138_3', 'Q138_4', 'Q138_5', 'Q140_1', 'Q140_2', 'Q140_3', 'Q140_4','Q140_5', 'Q141_1',\n",
    " 'Q141_2', 'Q141_3', 'Q141_4', 'Q141_5', 'Q142_1', 'Q142_2', 'Q142_3', 'Q142_4', 'Q142_5', 'Q143_1', 'Q143_2','Q143_3', \n",
    "          'Q143_4', 'Q143_5']\n",
    "# Factor 1- cognitive-sensitivity [CS] 14,17,19,24,21\n",
    "# Factor 2- emotional-sensitivity [ES]1,2, 5, 7, 9, 11\n",
    "# Factor 3- emotional-concern [EC]10, 15, 18, 20 (negatively poled), 22, 23, 25 \n",
    "# Factor 4- cognitive-concern[CC] 3, 4, 6, 8, 13\n",
    "cs=[14,17,19,24,21]\n",
    "es=[1,2, 5, 7, 9, 11]\n",
    "ec=[10, 15, 18, 20, 22, 23, 25 ]\n",
    "cc=[3, 4, 6, 8, 13]\n",
    "e_scale=[]\n",
    "for i in range(1,26):\n",
    "    if i in cs:\n",
    "        i=str(i)+\"-\"+'escale_cs'\n",
    "        e_scale.append(i)\n",
    "    if i in es:\n",
    "        i=str(i)+\"-\"+'escale_es'\n",
    "        e_scale.append(i)\n",
    "    if i in ec:\n",
    "        i=str(i)+\"-\"+'escale_ec'\n",
    "        e_scale.append(i)\n",
    "    if i in cc:\n",
    "        i=str(i)+\"-\"+'escale_cc'\n",
    "        e_scale.append(i)\n",
    "        \n",
    "escale_dict=dict(zip(raw_cols,e_scale))        \n",
    "raw_onlinefb=raw_onlinefb.rename(columns={'Q138_1': '1-escale_es', 'Q138_2': '2-escale_es', \n",
    "                                          'Q138_3': '3-escale_cc', 'Q138_4': '4-escale_cc', 'Q138_5': '5-escale_es', 'Q140_1': '6-escale_cc',\n",
    " 'Q140_2': '7-escale_es', 'Q140_3': '8-escale_cc','Q140_4': '9-escale_es', 'Q140_5': '10-escale_ec', \n",
    "                                          'Q141_1': '11-escale_es', 'Q141_2': '13-escale_cc',\n",
    " 'Q141_3': '14-escale_cs', 'Q141_4': '15-escale_ec', 'Q141_5': '17-escale_cs', 'Q142_1': '18-escale_ec',\n",
    "                                          'Q142_2': '19-escale_cs', 'Q142_3': '20-escale_ec',\n",
    " 'Q142_4': '21-escale_cs', 'Q142_5': '22-escale_ec', 'Q143_1': '23-escale_ec', 'Q143_2': '24-escale_cs',\n",
    "                                          'Q143_3': '25-escale_ec'})\n",
    "raw_onlinefb['20-escale_ec'] = raw_onlinefb['20-escale_ec'].map({1:5, 2:4, 3:3, 4:2,5:1})\n",
    "raw_onlinefb['e-scale_total']=raw_onlinefb[e_scale].sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#to remove the empty cols of avatar test and bringing all under each other\n",
    "\n",
    "\n",
    "#remove the cols from 1_Q194_First Click to 1_Q196_First Click\n",
    "\n",
    "\n",
    "\n",
    "# Replacing empty string with np.NaN\n",
    "import numpy as np\n",
    "#if vp has already fill the qs in deins oder meins drop it! \n",
    "raw_onlinefb=raw_onlinefb.loc[raw_onlinefb['Q208'] !=1]\n",
    "raw_onlinefb = raw_onlinefb.replace('', np.nan)\n",
    "df_left=raw_onlinefb[~raw_onlinefb['1_Q194_First Click'].isnull()]\n",
    "#remove the unwanted cols from df_left\n",
    "\n",
    "df_left.drop(columns=df_left.loc[:,'1_Q196_First Click':'64_Q193'], axis=1,inplace=True)\n",
    "#creating two separated dfs to get rid of duplicate cols\n",
    "df_right=raw_onlinefb[raw_onlinefb['1_Q194_First Click'].isnull()]\n",
    "\n",
    "#remove the unwanted cols from df_right\n",
    "df_right.drop(columns=df_right.loc[:,'1_Q194_First Click':'64_Q175'], axis=1,inplace=True)\n",
    "\n",
    "#df_left name of cols \n",
    "col_df_left=df_left.loc[:,'1_Q194_First Click':'64_Q175'].columns.to_list()\n",
    "#df_right name of cols\n",
    "col_df_right=df_right.loc[:,'1_Q196_First Click':'64_Q193'].columns.to_list()\n",
    "#make dict for both cols names\n",
    "dict_col_name=dict(zip(col_df_right,col_df_left))  \n",
    "#rename the df_right specific col names by df_left col names\n",
    "df_right=df_right.rename(dict_col_name,axis=1)\n",
    "#add the information which order of perspective they did: left or right\n",
    "df_left['perspective-order']='left'\n",
    "df_right['perspective-order']='right'\n",
    "#encode the right and wrong questions\n",
    "#for now I coded not answered 'NONE'qs as wrong answered question\n",
    "#left order 5=stimmt nicht überein 4=stimmt überein \n",
    "#right order 4=stimmt nicht überein , 5=stimmt überein \n",
    "#questions and correct answers\n",
    "\n",
    "#question_left_5 as correct answer\n",
    "ql5c=['1_Q175','5_Q175','7_Q175','8_Q175','13_Q175','14_Q175','18_Q175','19_Q175','21_Q175','22_Q175','23_Q175','30_Q175','31_Q175','35_Q175'\n",
    ",'36_Q175','38_Q175','39_Q175','40_Q175','41_Q175','43_Q175','46_Q175','48_Q175','51_Q175','52_Q175','53_Q175','55_Q175','56_Q175','57_Q175',\n",
    "'59_Q175','60_Q175','63_Q175','64_Q175']\n",
    "df_left[ql5c]= np.where(df_left[ql5c]==5, 0, 1)\n",
    "#question_right_4 as correct answer\n",
    "qr4c=['1_Q175','5_Q175','7_Q175','8_Q175','13_Q175','14_Q175','18_Q175','19_Q175','21_Q175','22_Q175','23_Q175','30_Q175','31_Q175','35_Q175'\n",
    ",'36_Q175','38_Q175','39_Q175','40_Q175','41_Q175','43_Q175','46_Q175','48_Q175','51_Q175','52_Q175','53_Q175','55_Q175','56_Q175','57_Q175',\n",
    "'59_Q175','60_Q175','63_Q175','64_Q175']\n",
    "df_right[qr4c]= np.where(df_right[qr4c]==4, 0, 1)\n",
    "#question_left_4 as correct answer\n",
    "ql4c=['2_Q175','3_Q175','4_Q175','6_Q175','9_Q175','10_Q175','11_Q175','12_Q175','15_Q175','16_Q175','17_Q175','20_Q175','24_Q175','25_Q175'\n",
    ",'26_Q175','27_Q175','28_Q175','29_Q175','32_Q175','33_Q175','34_Q175','37_Q175','42_Q175','44_Q175','45_Q175','47_Q175','49_Q175','50_Q175',\n",
    "'54_Q175','58_Q175','61_Q175','62_Q175']\n",
    "df_left[ql4c]= np.where(df_left[ql4c]==4, 0, 1)\n",
    "#question_right_5 as correct answer\n",
    "qr5c=['2_Q175','3_Q175','4_Q175','6_Q175','9_Q175','10_Q175','11_Q175','12_Q175','15_Q175','16_Q175','17_Q175','20_Q175','24_Q175','25_Q175'\n",
    ",'26_Q175','27_Q175','28_Q175','29_Q175','32_Q175','33_Q175','34_Q175','37_Q175','42_Q175','44_Q175','45_Q175','47_Q175','49_Q175','50_Q175',\n",
    "'54_Q175','58_Q175','61_Q175','62_Q175']\n",
    "df_right[qr5c]= np.where(df_right[qr5c]==5, 0, 1)\n",
    "# i need to create four groups 1. self congurent (sc) 2. self incongruent(si) 3.other congruent(oc) 4.other incongruent (oi)\n",
    "sc=['10_Q175','12_Q175','15_Q175','16_Q175','20_Q175','25_Q175','26_Q175','27_Q175','28_Q175','33_Q175','34_Q175','42_Q175','47_Q175','49_Q175'\n",
    ",'61_Q175','62_Q175']\n",
    "si=['1_Q175','7_Q175','14_Q175','18_Q175','21_Q175','31_Q175','36_Q175','38_Q175','41_Q175','46_Q175','51_Q175','56_Q175','59_Q175','60_Q175'\n",
    ",'63_Q175','64_Q175']\n",
    "oc=['2_Q175','3_Q175','4_Q175','6_Q175','9_Q175','11_Q175','17_Q175','24_Q175','29_Q175','32_Q175','37_Q175','44_Q175','45_Q175','50_Q175'\n",
    ",'54_Q175','58_Q175']\n",
    "oi=['5_Q175','8_Q175','13_Q175','19_Q175','22_Q175','23_Q175','30_Q175','35_Q175','39_Q175','40_Q175','43_Q175','48_Q175','52_Q175','53_Q175'\n",
    ",'55_Q175','57_Q175']\n",
    "\n",
    "#append both dfs\n",
    "df=df_left.append(df_right)\n",
    "#column for sum of sc\n",
    "df['self_congurent']= df.apply(lambda row: row[sc].sum(),axis=1)\n",
    "df['self_congurent']=(16-df['self_congurent'])/16\n",
    "#column for sum of si\n",
    "df['self_incongurent']=df.apply(lambda row: row[si].sum(),axis=1)\n",
    "df['self_incongurent']=(16-df['self_incongurent'])/16\n",
    "#column for sum of oc\n",
    "df['other_congurent']=df.apply(lambda row: row[oc].sum(),axis=1)\n",
    "df['other_congurent']=(16-df['other_congurent'])/16\n",
    "#column for sum of oi\n",
    "df['other_incongurent']=df.apply(lambda row: row[oi].sum(),axis=1)\n",
    "df['other_incongurent']=(16-df['other_incongurent'])/16\n",
    "#calculate the incongurent! if value is >0 means self incongurent and if <0 other ingurent and 0 means no ingurennt\n",
    "df['perspective']=(df['self_incongurent']-df['self_congurent'])-(df['other_incongurent']-df['other_congurent'])\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "df.rename(columns={'StartDate':'Date','Q132_1':'Vp-Code'},inplace=True)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "#export only necessary cols for statistical analysis\n",
    "cols_wanted=['Date','Duration (in seconds)','Vp-Code','TICS', 'TICS_control1', 'TICS_control2', 'TICS_control3', 'SVO_Final','BIS_Total_qs', 'BAS_total_qs', 'BAS_drive', 'BAS_FUN', 'BAS_rewards', 'BIS_15_MI', 'BIS_15_NPI', 'BIS_15_ABI',\n",
    " 'BIS_15_total', 'BFI_N', 'BFI_E', 'BFI_O', 'BFI_A', 'BFI_C', 'crt_ball', 'crt_machine', 'crt_lake', 'stai_total', 'sds_total',\n",
    " 'meq_total', 'e-scale_total', 'perspective']\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "df=df[cols_wanted]\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "#export the edited DF\n",
    "df_append=pd.ExcelWriter('df_online_luca.xlsx')\n",
    "df.to_excel(df_append)\n",
    "df_append.save()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Panas\n",
    "\n",
    "#import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import the raw data\n",
    "panas_raw=pd.read_excel('panas_raw.xlsx')\n",
    "panas_raw.rename(columns={'VP-Code':'Vp-Code'},inplace=True)\n",
    "\n",
    "#remove fake rows\n",
    "# list the rows, see which one is fake\n",
    "\n",
    "\n",
    "panas_raw = panas_raw.drop(panas_raw[panas_raw['Vp-Code'].isin(['sdewewa','Karaviti Evdoxia ','BRHA12M','mg1122','TEST','Fake','1234','12345','test','Test'])].index)\n",
    "panas_raw = panas_raw.drop(panas_raw.index[[0,3]])\n",
    "\n",
    "\n",
    "#remove unwanted cols\n",
    "unwanted_cols=[\n",
    " 'Status', 'IPAddress','sex-sample1', 'Finished',  'Tablet-taken_stress1',\n",
    "    'RecordedDate', 'RecipientLastName',\n",
    " 'RecipientFirstName', 'RecipientEmail', 'ExternalReference', 'LocationLatitude',\n",
    " 'LocationLongitude', 'DistributionChannel','stress3', 'UserLanguage']\n",
    "panas_raw=panas_raw.drop(columns=unwanted_cols, axis=1)\n",
    "\n",
    "#change the name of COLS from stress after 40 min\n",
    "original_cols=['Q15_1',\n",
    " 'Q15_2', 'Q15_3', 'Q15_4', 'Q15_5', 'Q15_6', 'Q15_7', 'Q15_8', 'Q15_9',\n",
    " 'Q15_10', 'Q15_11', 'Q15_12', 'Q15_13','Q15_14',\n",
    " 'Q15_15', 'Q15_16', 'Q15_17', 'Q15_18', 'Q15_19', 'Q15_20']\n",
    "min_40=['PANAS-40min_1', 'PANAS-40min_2', 'PANAS-40min_3',\n",
    " 'PANAS-40min_4', 'PANAS-40min_5', 'PANAS-40min_6', 'PANAS-40min_7', 'PANAS-40min_8', \n",
    "'PANAS-40min_9', 'PANAS-40min_10', 'PANAS-40min_11', 'PANAS-40min_12',\n",
    " 'PANAS-40min_13', 'PANAS-40min_14', 'PANAS-40min_15',\n",
    " 'PANAS-40min_16', 'PANAS-40min_17', 'PANAS-40min_18', 'PANAS-40min_19', 'PANAS-40min_20']\n",
    "\n",
    "dict_panas=dict(zip(original_cols,min_40))\n",
    "panas_raw = panas_raw.rename(columns=dict_panas)\n",
    "\n",
    "#panas_raw.columns.to_list()\n",
    "#list of positive items\n",
    "#positive=[1,3,5,9,10,12,14,16,17,19]\n",
    "#negative=[2,4,6,7,8,11,13,15,18,20]\n",
    "##for PANAS in base\n",
    "#positive items\n",
    "positive_basic=['PANAS-basic_1','PANAS-basic_3','PANAS-basic_5','PANAS-basic_9',\n",
    "               'PANAS-basic_10','PANAS-basic_12','PANAS-basic_14','PANAS-basic_16',\n",
    "               'PANAS-basic_17','PANAS-basic_19']\n",
    "#negative items\n",
    "negative_basic=['PANAS-basic_2','PANAS-basic_11','PANAS-basic_13','PANAS-basic_15',\n",
    "               'PANAS-basic_8','PANAS-basic_12','PANAS-basic_14','PANAS-basic_16',\n",
    "               'PANAS-basic_18','PANAS-basic_20']\n",
    "#new col by calculating sum of positive and negative iitems\n",
    "panas_raw['positive_basic']=panas_raw[positive_basic].sum(axis=1)\n",
    "panas_raw['negative_basic']=panas_raw[negative_basic].sum(axis=1)\n",
    "\n",
    "\n",
    "#for PANAS after 20 minutes\n",
    "positive_20min=['PANAS-20min_1','PANAS-20min_3','PANAS-20min_5','PANAS-20min_9',\n",
    "               'PANAS-20min_10','PANAS-20min_12','PANAS-20min_14','PANAS-20min_16',\n",
    "               'PANAS-20min_17','PANAS-20min_19']\n",
    "negative_20min=['PANAS-20min_2','PANAS-20min_11','PANAS-20min_13','PANAS-20min_15',\n",
    "               'PANAS-20min_8','PANAS-20min_12','PANAS-20min_14','PANAS-20min_16',\n",
    "               'PANAS-20min_18','PANAS-20min_20']\n",
    "#new col by calculating sum of positive and negative iitems\n",
    "panas_raw['positive_20min']=panas_raw[positive_20min].sum(axis=1)\n",
    "panas_raw['negative_20min']=panas_raw[negative_20min].sum(axis=1)\n",
    "\n",
    "#for PANAS after 40 minutes\n",
    "positive_40min=['PANAS-40min_1','PANAS-40min_3','PANAS-40min_5','PANAS-40min_9',\n",
    "               'PANAS-40min_10','PANAS-40min_12','PANAS-40min_14','PANAS-40min_16',\n",
    "               'PANAS-40min_17','PANAS-40min_19']\n",
    "negative_40min=['PANAS-40min_2','PANAS-40min_11','PANAS-40min_13','PANAS-40min_15',\n",
    "               'PANAS-40min_8','PANAS-40min_12','PANAS-40min_14','PANAS-40min_16',\n",
    "               'PANAS-40min_18','PANAS-40min_20']\n",
    "#new col by calculating sum of positive and negative iitems\n",
    "panas_raw['positive_40min']=panas_raw[positive_40min].sum(axis=1)\n",
    "panas_raw['negative_40min']=panas_raw[negative_40min].sum(axis=1)\n",
    "\n",
    "#adding cols of stress levels\n",
    "panas_raw['basic_stress_level']=panas_raw['stress-basic_1']\n",
    "panas_raw['20min_stress_level']=panas_raw['stress-20min_1']\n",
    "panas_raw['40_stress_level']=panas_raw['stress-40min_1']\n",
    "#drop empty cel\n",
    "\n",
    "\n",
    "panas_raw['Vp-Code']=panas_raw['Vp-Code'].dropna()\n",
    "\n",
    "\n",
    "#export the whole panas df\n",
    "df_panas=pd.ExcelWriter('df_panas.xlsx')\n",
    "panas_raw.to_excel(df_panas)\n",
    "df_panas.save()\n",
    "\n",
    "\n",
    "panas_raw.columns.to_list()\n",
    "selected_cols=['StartDate', 'EndDate', 'Progress', 'Duration (in seconds)',\n",
    " 'ResponseId','Vp-Code', 'positive_basic', 'negative_basic',\n",
    " 'positive_20min', 'negative_20min', 'positive_40min',\n",
    " 'negative_40min', 'basic_stress_level', '20min_stress_level', '40_stress_level']\n",
    "selected_panas=panas_raw[selected_cols]\n",
    "#since first two VPs did not have panas, I should add their rows\n",
    "#all with no data. To have consistent data in all dataframes.\n",
    "ANHA19E={'StartDate':'NO_DATA', 'EndDate':'NO_DATA' ,'Progress':'NO_DATA',\n",
    "         'Duration (in seconds)':'NO_DATA',\n",
    " 'ResponseId':'NO_DATA','Vp-Code':'ANHA19E', 'positive_basic':'NO_DATA',\n",
    "         'negative_basic':'NO_DATA',\n",
    " 'positive_20min':'NO_DATA', 'negative_20min':'NO_DATA', 'positive_40min':'NO_DATA',\n",
    " 'negative_40min':'NO_DATA', 'basic_stress_level':'NO_DATA',\n",
    "         '20min_stress_level':'NO_DATA', '40_stress_level':'NO_DATA'}\n",
    "PASS13D={'StartDate':'NO_DATA', 'EndDate':'NO_DATA' ,\n",
    "         'Progress':'NO_DATA','Duration (in seconds)':'NO_DATA',\n",
    " 'ResponseId':'NO_DATA','Vp-Code':'PASS13D','positive_basic':'NO_DATA',\n",
    "         'negative_basic':'NO_DATA','positive_20min':'NO_DATA', \n",
    "         'negative_20min':'NO_DATA', 'positive_40min':'NO_DATA',\n",
    " 'negative_40min':'NO_DATA', 'basic_stress_level':'NO_DATA',\n",
    "         '20min_stress_level':'NO_DATA', '40_stress_level':'NO_DATA'}\n",
    "\n",
    "selected_panas=panas_raw[selected_cols]\n",
    "#first VP\n",
    "selected_panas=selected_panas.append(ANHA19E,ignore_index=True)\n",
    "#second VP\n",
    "selected_panas=selected_panas.append(PASS13D,ignore_index=True)\n",
    "selected_panas['Vp-Code'].dropna(how='any',inplace=True)\n",
    "sel_panas=pd.ExcelWriter('sel_panas.xlsx')\n",
    "selected_panas.to_excel(sel_panas)\n",
    "sel_panas.save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "<ipython-input-38-19d976a50a36>:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_iat['Intimacy Items_2']=df_iat['Intimacy Items_2'].map({4:1,3:2,2:3,1:4})\n",
      "<ipython-input-38-19d976a50a36>:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_iat['Intimacy Items_3']=df_iat['Intimacy Items_3'].map({4:1,3:2,2:3,1:4})\n",
      "<ipython-input-38-19d976a50a36>:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_iat['Intimacy Items_4']=df_iat['Intimacy Items_4'].map({4:1,3:2,2:3,1:4})\n",
      "<ipython-input-38-19d976a50a36>:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_iat['blatant_total']=df_iat[blatant_cols].sum(axis=1)\n",
      "<ipython-input-38-19d976a50a36>:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_iat['anti_intimacy_total']=df_iat[anti_intimacy_cols].sum(axis=1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "#DOM Tasks\n",
    "\n",
    "\n",
    "df_raw_dom=pd.read_excel('Dom_raw.xlsx')\n",
    "\n",
    "#cleaning->removing unwanted rows and cols\n",
    "#row\n",
    "df_raw_dom=df_raw_dom.drop(df_raw_dom.index[0])\n",
    "#cols\n",
    "df_raw_dom=df_raw_dom.drop(columns=['Status', 'IPAddress',\n",
    " 'Finished', 'RecordedDate',\n",
    " 'RecipientLastName', 'RecipientFirstName', 'RecipientEmail', 'ExternalReference',\n",
    " 'LocationLatitude', 'LocationLongitude', 'DistributionChannel', 'UserLanguage'], axis=1)\n",
    "#rename the vp code col\n",
    "df_raw_dom=df_raw_dom.rename(columns={'Q1':'Vp-Code'})\n",
    "#drop unwanted rows\n",
    "df_raw_dom = df_raw_dom.drop(df_raw_dom[df_raw_dom['Vp-Code'].isin(['Fake','FAKE','1234','12345','test','TEST'])].index)\n",
    "df_raw_dom = df_raw_dom.drop(df_raw_dom.index[0])\n",
    "df_raw_dom=df_raw_dom.drop(columns=['Q_RecaptchaScore'], axis=1)\n",
    "\n",
    "\n",
    "df_sd=df_raw_dom.loc[:,'StartDate':'Give_100']\n",
    "\n",
    "\n",
    "\n",
    "df_iat = df_raw_dom[list(df_raw_dom.columns[0:6]) + list(df_raw_dom.columns[34:44])]\n",
    "\n",
    "\n",
    "#calculating blatant and subtle prejudice\n",
    "blatant_cols=['Threat-Rejection-1','Threat-Rejection-2', \n",
    "              'Threat-Rejection-3', 'Threat-Rejection-4',\n",
    "              'Threat-Rejection-5', 'Threat-Rejection-6']\n",
    "\n",
    "anti_intimacy_cols=['Intimacy Items_1', 'Intimacy Items_2', 'Intimacy Items_3',\n",
    " 'Intimacy Items_4']\n",
    "blatant_dict={'Stimme voll zu':4,'Stimme teilweise zu':3,\n",
    "             'Weder noch':2,'Stimme  teilweise nicht zu':1,\n",
    "             'Stimme überhaupt nicht zu':0}\n",
    "#replace str with int\n",
    "df_iat.loc[:,'Threat-Rejection-1':'Threat-Rejection-6'].replace({'Stimme voll zu':4,'Stimme teilweise zu':3,\n",
    "             'Weder noch':2,'Stimme  teilweise nicht zu':1,'Stimme teilweise nicht zu':1,\n",
    "             'Stimme überhaupt nicht zu':0},inplace=True)\n",
    "#replace str with int\n",
    "df_iat.loc[:,'Intimacy Items_1':'Intimacy Items_4'].replace({'Stimme voll zu':4,'Stimme teilweise zu':3,\n",
    "                                                             'Weder noch':2,'Stimme teilweise nicht zu':1,\n",
    "                                                            'Stimme überhaupt nicht zu':0},inplace=True)\n",
    "#the first question of anti_intimacy should stay as it is! \n",
    "#these questions should be reversed.\n",
    "df_iat['Intimacy Items_2']=df_iat['Intimacy Items_2'].map({4:1,3:2,2:3,1:4})\n",
    "df_iat['Intimacy Items_3']=df_iat['Intimacy Items_3'].map({4:1,3:2,2:3,1:4})\n",
    "df_iat['Intimacy Items_4']=df_iat['Intimacy Items_4'].map({4:1,3:2,2:3,1:4})\n",
    "#adding a new col, total of each balant and anti intimacy items\n",
    "df_iat['blatant_total']=df_iat[blatant_cols].sum(axis=1)\n",
    "df_iat['anti_intimacy_total']=df_iat[anti_intimacy_cols].sum(axis=1)\n",
    "#remove the rows in vp_code that are test or by fake VPs\n",
    "not_wanted_rows=['Fake','fake','test','TEST','FAKE','Test','richard'\n",
    "                ,'Evdoxia','Evdoxia Karaviti']\n",
    "df_iat=df_iat[~df_iat['Vp-Code'].isin(not_wanted_rows)]\n",
    "#to add the missing VP code of first 5 rows\n",
    "df_iat.at[2,'Vp-Code']='ANHA19E'\n",
    "df_iat.at[3,'Vp-Code']='PAAS13D'\n",
    "df_iat.at[4,'Vp-Code']='GAJO22B'\n",
    "df_iat.at[5,'Vp-Code']='TAST22E'\n",
    "df_iat.at[6,'Vp-Code']='Heto16B'\n",
    "#drop the duplicate HERA27K\n",
    "df_iat.drop(12,axis=0, inplace=True)\n",
    "\n",
    "df_iat.sort_values(by=['ResponseId'], inplace=True)\n",
    "#import the iat from iatgen which is in csv\n",
    "iatgen=pd.read_csv('iat-results-3.csv',keep_default_na=False)\n",
    "\n",
    "iatgen.rename(columns={'ResponseID':'ResponseId'},inplace=True)\n",
    "iatgen.sort_values(by=['ResponseId'], inplace=True)\n",
    "\n",
    "df_iat=df_iat.merge(iatgen, on='ResponseId')\n",
    "#drop the unwanted col from iatgen df\n",
    "df_iat.drop(columns=['idid'],axis=1,inplace=True)\n",
    "\n",
    "#export the edited iat_DF\n",
    "df_iat1=pd.ExcelWriter('df_iat.xlsx')\n",
    "df_iat.to_excel(df_iat1)\n",
    "df_iat1.save()\n",
    "\n",
    "#to clean and export the IPD-MD task data\n",
    "df_ipd = df_raw_dom[list(df_raw_dom.columns[0:6]) + list(df_raw_dom.columns[66:80])]\n",
    "ipd_change_col_name=['10-euro_1', '10-euro_2', '10-euro_3',\n",
    " 'Q38' ,'Q39', '8-euro_1', '8-euro_2', '8-euro_3', 'Q40',\n",
    " 'Q42', '5-euro_1', '5-euro_2', '5-euro_3', 'Q41', 'Q43']\n",
    "ipd_new_name=['no_lost_A','no_lost_B','no_lost_C','no_lost_ingroup',\n",
    "              'no_lost_outgroup','20%_lost_A','20%_lost_B','20%_lost_C',\n",
    "              '20%_lost_ingroup','20%_lost_outgroup','50%_lost_A','50%_lost_B','50%_lost_C','50%_lost_ingroup',\n",
    "              '50%_lost_outgroup']\n",
    "dict_ipd=dict(zip(ipd_change_col_name,ipd_new_name))\n",
    "df_ipd=df_ipd.rename(columns=dict_ipd)\n",
    "#to add the missing VP code of first 5 rows\n",
    "df_ipd.at[2,'Vp-Code']='ANHA19E'\n",
    "df_ipd.at[3,'Vp-Code']='PAAS13D'\n",
    "df_ipd.at[4,'Vp-Code']='GAJO22B'\n",
    "df_ipd.at[5,'Vp-Code']='TAST22E'\n",
    "df_ipd.at[6,'Vp-Code']='Heto16B'\n",
    "#drop the duplicate HERA27K\n",
    "df_ipd.drop(12,axis=0, inplace=True)\n",
    "#remove the rows in vp_code that are test or by fake VPs\n",
    "not_wanted_rows=['Fake','fake','test','TEST','FAKE','Test','richard'\n",
    "                ,'Evdoxia','Evdoxia Karaviti']\n",
    "df_ipd=df_ipd[~df_ipd['Vp-Code'].isin(not_wanted_rows)]\n",
    "\n",
    "#calculate the percent of moey for each condition of IPD\n",
    "lost_no=['no_lost_A','no_lost_B','no_lost_C']\n",
    "lost_20=['20%_lost_A','20%_lost_B','20%_lost_C']\n",
    "lost_50=['50%_lost_A','50%_lost_B','50%_lost_C']\n",
    "df_ipd[lost_no]=df_ipd[lost_no]/10*100\n",
    "df_ipd[lost_20]=df_ipd[lost_20]/8*100\n",
    "df_ipd[lost_50]=df_ipd[lost_50]/5*100\n",
    "\n",
    "#export the edited IPDMD\n",
    "df_ipd_excel=pd.ExcelWriter('df_ipd.xlsx')\n",
    "df_ipd.to_excel(df_ipd_excel)\n",
    "df_ipd_excel.save()\n",
    "\n",
    "#to clean and export the demographical data\n",
    "df_demog= df_raw_dom[list(df_raw_dom.columns[0:6]) + list(df_raw_dom.columns[104:])]\n",
    "#change the name of cols [ q7 and q8]\n",
    "df_demog.rename(columns={'Q7':'Sex','Q8':'Age'},inplace=True)\n",
    "#to add the missing VP code of first 5 rows\n",
    "df_demog.at[2,'Vp-Code']='ANHA19E'\n",
    "df_demog.at[3,'Vp-Code']='PAAS13D'\n",
    "df_demog.at[4,'Vp-Code']='GAJO22B'\n",
    "df_demog.at[5,'Vp-Code']='TAST22E'\n",
    "df_demog.at[6,'Vp-Code']='Heto16B'\n",
    "#remove the rows in vp_code that are test or by fake VPs\n",
    "not_wanted_rows=['Fake','fake','test','TEST','FAKE','Test','richard'\n",
    "                ,'Evdoxia','Evdoxia Karaviti']\n",
    "df_demog=df_demog[~df_demog['Vp-Code'].isin(not_wanted_rows)]\n",
    "\n",
    "#change the cols of demographic qs to text \n",
    "#q1\n",
    "d1=[]\n",
    "for i in df_demog['Debriefing-Q1']:\n",
    "    if i==1:\n",
    "        i='Placebo'\n",
    "        d1.append(i)\n",
    "    else:\n",
    "        i='Drug'\n",
    "        d1.append(i)\n",
    "df_demog['Debriefing-Q1']=d1\n",
    "#q2\n",
    "df_demog['Debriefing-Q2'].fillna(value=5,inplace=True)\n",
    "d2=[]\n",
    "for i in df_demog['Debriefing-Q2']:\n",
    "    if i==1:\n",
    "        i='Hydrocortison'\n",
    "        d2.append(i)\n",
    "    if i==2:\n",
    "        i='Yohimbine'\n",
    "        d2.append(i)\n",
    "    if i==3:\n",
    "        i='H+Y'\n",
    "        d2.append(i)\n",
    "    if i==4:\n",
    "        i='Placebo'\n",
    "        d2.append(i)\n",
    "    if i==5:\n",
    "        i='No-info'\n",
    "        d2.append(i)\n",
    "        \n",
    "df_demog['Debriefing-Q2']=d2\n",
    "df_demog.rename(columns={'Debriefing-Q2':'Which_drug?'},inplace=True)\n",
    "#q4\n",
    "d4=[]\n",
    "for i in df_demog['Debriefing-Q4']:\n",
    "    if i==1:\n",
    "        i='YES'\n",
    "        d4.append(i)\n",
    "    else:\n",
    "        i='No'\n",
    "        d4.append(i)\n",
    "df_demog['Debriefing-Q4']=d4\n",
    "df_demog.rename(columns={'Debriefing-Q4':'effect_on_your_behavior?'},inplace=True)\n",
    "#'Demo_nationality'\n",
    "#there is one nan that I will change to 6 which means no info\n",
    "df_demog['Demo_nationality'].fillna(value=6,inplace=True)\n",
    "nationality=[]\n",
    "for i in df_demog['Demo_nationality']:\n",
    "    if i==1:\n",
    "        i='German'\n",
    "        nationality.append(i)\n",
    "    if i==2:\n",
    "        i='Turkish'\n",
    "        nationality.append(i)\n",
    "    if i==3:\n",
    "        i='Polnish'\n",
    "        nationality.append(i)\n",
    "    if i==4:\n",
    "        i='Syrian'\n",
    "        nationality.append(i)\n",
    "    if i==5:\n",
    "        i='Other'\n",
    "        nationality.append(i)\n",
    "    if i==6:\n",
    "        i='No_Info'\n",
    "        nationality.append(i)\n",
    "df_demog['Demo_nationality']=nationality\n",
    "#education\n",
    "#there is one nan that I will change to 6 which means no info\n",
    "df_demog['Demo_education'].fillna(value=9,inplace=True)\n",
    "education=[]\n",
    "for i in df_demog['Demo_education']:\n",
    "    if i==1:\n",
    "        i='primary-incomplete'\n",
    "        education.append(i)\n",
    "    if i==2:\n",
    "        i='primary-complete'\n",
    "        education.append(i)\n",
    "    if i==3:\n",
    "        i='middle school-incomplete'\n",
    "        education.append(i)\n",
    "    if i==4:\n",
    "        i='middle-school-complete'\n",
    "        education.append(i)\n",
    "    if i==5:\n",
    "        i='technical student'\n",
    "        education.append(i)\n",
    "    if i==6:\n",
    "        i='uni student'\n",
    "        education.append(i)\n",
    "    if i==7:\n",
    "        i='Bachelor'\n",
    "        education.append(i)\n",
    "    if i==8:\n",
    "        i='master_or_phd'\n",
    "        education.append(i)\n",
    "    if i==9:\n",
    "        i='No_info'\n",
    "        education.append(i)\n",
    "df_demog['Demo_education']=education\n",
    "#income\n",
    "df_demog['Demo_income'].fillna(value=6,inplace=True)\n",
    "income=[]\n",
    "for i in df_demog['Demo_income']:\n",
    "    if i==1:\n",
    "        i='<25.000'\n",
    "        income.append(i)\n",
    "    if i==2:\n",
    "        i='25.000-50.000'\n",
    "        income.append(i)\n",
    "    if i==3:\n",
    "        i='50.000-100.000'\n",
    "        income.append(i)\n",
    "    if i==4:\n",
    "        i='100.000-200.000'\n",
    "        income.append(i)\n",
    "    if i==5:\n",
    "        i='>200.000'\n",
    "        income.append(i)\n",
    "    if i==6:\n",
    "        i='No_Info'\n",
    "        income.append(i)\n",
    "df_demog['Demo_income']=income\n",
    "#employment\n",
    "df_demog['Demo_employment'].fillna(value=6,inplace=True)\n",
    "employment=[]\n",
    "for i in df_demog['Demo_employment']:\n",
    "    if i==1:\n",
    "        i='working'\n",
    "        employment.append(i)\n",
    "    if i==2:\n",
    "        i='student'\n",
    "        employment.append(i)\n",
    "    if i==3:\n",
    "        i='job searcher'\n",
    "        employment.append(i)\n",
    "    if i==4:\n",
    "        i='other'\n",
    "        employment.append(i)\n",
    "    if i==5:\n",
    "        i='no-info'\n",
    "        employment.append(i)\n",
    "    if i==6:\n",
    "        i='student'\n",
    "        employment.append(i)\n",
    "    if i==7:\n",
    "        i='no-info'\n",
    "        employment.append(i)\n",
    "    if i==8:\n",
    "        i='no-info'\n",
    "employment.append(i)\n",
    "df_demog['Demo_employment']=employment\n",
    "#Q7\n",
    "df_demog['Sex'].fillna(value=4,inplace=True)\n",
    "sex1=[]\n",
    "for i in df_demog['Sex']:\n",
    "    if i==1:\n",
    "        i='F'\n",
    "        sex1.append(i)\n",
    "    if i==2:\n",
    "        i='M'\n",
    "        sex1.append(i)\n",
    "    if i==3:\n",
    "        i='Other'\n",
    "        sex1.append(i)\n",
    "    if i==4:\n",
    "        i='no-info'\n",
    "        sex1.append(i)\n",
    "df_demog['Sex']=sex1\n",
    "df_demog.drop(12,axis=0, inplace=True)\n",
    "df_demog_xls=pd.ExcelWriter('df_demog_xls.xlsx')\n",
    "df_demog.to_excel(df_demog_xls)\n",
    "df_demog_xls.save()\n",
    "\n",
    "#to create the data of social discounting. there are one social discounting \n",
    "#from september which is for the previous version of the sd task\n",
    "sd_sep=pd.read_excel('Copy of YOM_7 September 2022_13.30.xlsx')\n",
    "#the data from the new version of the sd task\n",
    "sd_new=pd.read_excel('Dom_raw.xlsx')\n",
    "#columns of sd data\n",
    "sd_columns=['ResponseId','StartDate','EndDate','Q1','Taken_1', 'Taken_2', 'Taken_3', 'Taken_5',\n",
    " 'Taken_10 ', 'Taken_20', 'Taken_50 ', 'Taken_100', 'Give_1 ',\n",
    " 'Give_2', 'Give_3', 'Give_5', 'Give_10', 'Give_20', 'Give_50', 'Give_100']\n",
    "sd_sep=sd_sep[sd_columns]\n",
    "sd_sep=sd_sep.drop(index=0)\n",
    "sd_sep=sd_sep.rename(columns={'Q1':'Vp-Code'})\n",
    "sd_sep=sd_sep[~sd_sep['Vp-Code'].isin(not_wanted_rows)]\n",
    "sd_new=sd_new[sd_columns]\n",
    "sd_new=sd_new.drop(index=range(0,32))\n",
    "sd_new=sd_new.rename(columns={'Q1':'Vp-Code'})\n",
    "not_wanted_rows=['Fake','fake','test','TEST','FAKE','Test','richard'\n",
    "                ,'Evdoxia','Evdoxia Karaviti']\n",
    "sd_new=sd_new[~sd_new['Vp-Code'].isin(not_wanted_rows)]\n",
    "#append two dfs\n",
    "sd_df=sd_sep.append(sd_new)\n",
    "sd_df=sd_df.drop(index=[1,12])\n",
    "#adding the vp names to those did not have. because at first days we did\n",
    "#not ask vps to enter their vps codes in DOM survey\n",
    "sd_df.at[2,'Vp-Code']='ANHA19E'\n",
    "sd_df.at[3,'Vp-Code']='PAAS13D'\n",
    "sd_df.at[4,'Vp-Code']='GAJO22B'\n",
    "sd_df.at[5,'Vp-Code']='TAST22E'\n",
    "sd_df.at[6,'Vp-Code']='Heto16B'\n",
    "\n",
    "\n",
    "df_sd1=pd.ExcelWriter('df_sd.xlsx')\n",
    "sd_df.to_excel(df_sd1)\n",
    "df_sd1.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:3887: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py:643: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Mergeing all datasets\n",
    "import pandas as pd\n",
    "#first concat dfs from two online QS\n",
    "online_dom=pd.read_excel('df_online_DOM.xlsx')\n",
    "online_luca=pd.read_excel('df_online_luca.xlsx')\n",
    "df_dom_luca=[online_dom,online_luca]\n",
    "online_df=pd.concat(df_dom_luca)\n",
    "#drop the vps that are duplicate\n",
    "online_df.drop_duplicates(subset=['Vp-Code'], inplace=True)\n",
    "#drop the vps of tests\n",
    "test_vps=['Finja']\n",
    "online_df=online_df[online_df['Vp-Code'].isin(test_vps)==False]\n",
    "#There is no Online Data for this VP\n",
    "MAPI19B={'Unnamed: 0':'No_DATA', 'Unnamed: 0.1':'No_DATA', 'Date':'No_DATA', 'Duration (in seconds)':'No_DATA', 'Vp-Code':'MAPI29B',\n",
    " 'TICS':'No_DATA', 'TICS_control1':'No_DATA', 'TICS_control2':'No_DATA', 'TICS_control3':'No_DATA', 'SVO_Final':'No_DATA', 'BIS_Total_qs':'No_DATA', 'BAS_total_qs':'No_DATA',\n",
    " 'BAS_drive':'No_DATA', 'BAS_FUN':'No_DATA', 'BAS_rewards':'No_DATA', 'BIS_15_MI':'No_DATA', 'BIS_15_NPI':'No_DATA', 'BIS_15_ABI':'No_DATA', 'BIS_15_total':'No_DATA', \n",
    "         'BFI_N':'No_DATA', 'BFI_E':'No_DATA', 'BFI_O':'No_DATA', 'BFI_A':'No_DATA', 'BFI_C':'No_DATA', 'crt_ball':'No_DATA', 'crt_machine':'No_DATA',\n",
    "         'crt_lake':'No_DATA', 'stai_total':'No_DATA', 'sds_total':'No_DATA', 'meq_total':'No_DATA',\n",
    " 'e-scale_total':'No_DATA', 'perspective':'No_DATA'}\n",
    "\n",
    "online_df=online_df.append(MAPI19B,ignore_index=True)\n",
    "#export the edited DF\n",
    "df_concat=pd.ExcelWriter('df_online_concat.xlsx')\n",
    "online_df.to_excel(df_concat)\n",
    "df_concat.save()\n",
    "\n",
    "#first I will merge four DFs of with this order: PANAS-IPD-IAT-SD-Demog\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "#data of online QS\n",
    "online_df=pd.read_excel('df_online_concat.xlsx')\n",
    "#data of stress+panas\n",
    "panas=pd.read_excel('sel_panas.xlsx',keep_default_na=False)\n",
    "#sort based on by col!\n",
    "panas.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#data of IPD-MD task\n",
    "ipd=pd.read_excel('df_ipd.xlsx')\n",
    "\n",
    "#sort based on by col!\n",
    "ipd.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#data of social discounting task\n",
    "sd=pd.read_excel('df_sd.xlsx')\n",
    "#sort based on by col!\n",
    "sd.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#data of iat task\n",
    "iat=pd.read_excel('df_iat.xlsx')\n",
    "#sort based on by col!\n",
    "iat.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#merge fourt DFs. name of df is based in first letter of each df\n",
    "pisi=[panas,ipd,sd,iat]\n",
    "\n",
    "#as there is not exact values in Vp-code col, I need to merge them\n",
    "#multiple times\n",
    "from fuzzywuzzy import process\n",
    "# create a choice list\n",
    "choices = panas['Vp-Code'].values.tolist()\n",
    "\n",
    "# apply fuzzywuzzy to each row using lambda expression\n",
    "sd['Vp-Code'] = sd['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "sd[['Vp-Code', 'Percent Match']] = sd['Vp-Code'].apply(pd.Series)\n",
    "\n",
    "# merge\n",
    "sd_pa=sd.merge(panas, left_on='Vp-Code', right_on='Vp-Code')\n",
    "\n",
    "#now merge ipd to the above already merged df\n",
    "choices = ipd['Vp-Code'].values.tolist()\n",
    "\n",
    "# apply fuzzywuzzy to each row using lambda expression\n",
    "sd_pa['Vp-Code'] = sd_pa['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "sd_pa[['Vp-Code', 'Percent Match']] = sd_pa['Vp-Code'].apply(pd.Series)\n",
    "\n",
    "\n",
    "# merge\n",
    "sd_pa_ipd=sd_pa.merge(ipd, left_on='Vp-Code', right_on='Vp-Code')\n",
    "\n",
    "#now merge iat to the above already merged df\n",
    "choices = iat['Vp-Code'].values.tolist()\n",
    "\n",
    "# apply fuzzywuzzy to each row using lambda expression\n",
    "sd_pa_ipd['Vp-Code'] = sd_pa_ipd['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "sd_pa_ipd[['Vp-Code', 'Percent Match']] = sd_pa_ipd['Vp-Code'].apply(pd.Series)\n",
    "\n",
    "\n",
    "# merge\n",
    "sd_pa_ipd_iat=sd_pa_ipd.merge(iat, left_on='Vp-Code', right_on='Vp-Code')\n",
    "\n",
    "#add the data of online questionnires (concat one)\n",
    "choices = online_df['Vp-Code'].values.tolist()\n",
    "\n",
    "sd_pa_ipd_iat['Vp-Code'] = sd_pa_ipd_iat['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "sd_pa_ipd_iat[['Vp-Code', 'Percent Match']] = sd_pa_ipd_iat['Vp-Code'].apply(pd.Series)\n",
    "\n",
    "\n",
    "# merge\n",
    "online_sd_pa_ipd_iat=sd_pa_ipd_iat.merge(online_df, left_on='Vp-Code', right_on='Vp-Code')\n",
    "\n",
    "online_sd_pa_ipd_iat = online_sd_pa_ipd_iat[online_sd_pa_ipd_iat['Percent Match']>75]\n",
    "#drop the cols that are not necessary\n",
    "online_sd_pa_ipd_iat=online_sd_pa_ipd_iat.drop(columns=['Unnamed: 0_x','Unnamed: 0_y','Unnamed: 0_x',\n",
    "                                                       'Unnamed: 0_y','Unnamed: 0.1.1'], axis=1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_samples_raw=pd.read_excel('DOM-Samples-Raw.xlsx')\n",
    "#group by data to have hearth beat for each six times of measurements\n",
    "df_samples_raw=df_samples_raw.pivot(index=['Vp-Code','Condition','Gender'], columns=['Samples_order'], values='Heartbeat')\n",
    "\n",
    "#convert pivot table to dataframe\n",
    "df_samples_raw=df_samples_raw.reset_index()\n",
    "#change the order of columns to make the sample orders righ\n",
    "cols=['Vp-Code', 'Condition','Base', '20_minute', '40_minute',  'First_task', 'Second_task', 'Third_taks','Gender']\n",
    "df_samples_raw=df_samples_raw[cols]\n",
    "#df_samples_raw=df_samples_raw.rename({'Base':'HB-Base', '20_minute':'HB-20_minute', '40_minute':'HB-40_minute', \n",
    "                                     # 'First_task':'HB-First_task','Second_task':'HB-Second_task', 'Third_taks':'HB-Third_taks'})\n",
    "#sort the data on Vp-Code\n",
    "df_samples_raw.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#so far I have not added the sex and stress samples results\n",
    "#need to add the samples data to combined df\n",
    "#add the data of samples\n",
    "choices = df_samples_raw['Vp-Code'].values.tolist()\n",
    "\n",
    "online_sd_pa_ipd_iat['Vp-Code'] = online_sd_pa_ipd_iat['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "online_sd_pa_ipd_iat[['Vp-Code', 'Percent Match']] = online_sd_pa_ipd_iat['Vp-Code'].apply(pd.Series)\n",
    "\n",
    "\n",
    "# merge\n",
    " \n",
    "Dom_final=online_sd_pa_ipd_iat.merge(df_samples_raw, left_on='Vp-Code', right_on='Vp-Code')\n",
    "\n",
    "\n",
    "#add df of sex and stress samples analysis\n",
    "sex=pd.read_excel('sex_stress samples.xlsx', sheet_name='sex')\n",
    "stress=pd.read_excel('sex_stress samples.xlsx', sheet_name='stress')\n",
    "#drop unwanted cols\n",
    "wcols=['participant code', 'Testosterone pg/ml', 'Progesterone pg/ml', 'Estradiol pg/ml']\n",
    "sex=sex[wcols]\n",
    "#rename the col\n",
    "sex=sex.rename(columns={'participant code':'Vp-Code'})\n",
    "\n",
    "#sort the data on vp code\n",
    "sex.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#pivot table to show eacg time of stress sample analysis separately\n",
    "df_stress=stress.pivot(index=['Vp-Code','Condition'], columns=['Samples_order'], values=['Yohimbine','hydrocortisone'])\n",
    "#convert pivot table to dataframe\n",
    "df_stress=df_stress.reset_index()\n",
    "df_stress.sort_values(by=['Vp-Code'], inplace=True)\n",
    "#drop the condition col\n",
    "df_stress.drop(columns=('Condition'),inplace=True)\n",
    "#sort the on vp code\n",
    "\n",
    "#first merge sex sample\n",
    "choices = sex['Vp-Code'].values.tolist()\n",
    "\n",
    "Dom_final['Vp-Code'] = Dom_final['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "Dom_final[['Vp-Code', 'Percent Match']] = Dom_final['Vp-Code'].apply(pd.Series)\n",
    "\n",
    "\n",
    "# merge\n",
    " \n",
    "Dom_final=Dom_final.merge(sex, left_on='Vp-Code', right_on='Vp-Code')#\n",
    "\n",
    "#second merge the stress sample analysis\n",
    "choices = df_stress['Vp-Code'].values.tolist()\n",
    "\n",
    "Dom_final['Vp-Code'] = Dom_final['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "Dom_final[['Vp-Code', 'Percent Match']] = Dom_final['Vp-Code'].apply(pd.Series)\n",
    "# merge\n",
    "Dom_final=Dom_final.merge(df_stress, left_on='Vp-Code', right_on='Vp-Code')#\n",
    "#add the demographical output\n",
    "\n",
    "choices = df_demog['Vp-Code'].values.tolist()\n",
    "\n",
    "Dom_final['Vp-Code'] = Dom_final['Vp-Code'].apply(lambda x: process.extractOne(x, choices))\n",
    "# add percent match wiht apply\n",
    "Dom_final[['Vp-Code', 'Percent Match']] = Dom_final['Vp-Code'].apply(pd.Series)\n",
    "# merge\n",
    "Dom_final=Dom_final.merge(df_demog, left_on='Vp-Code', right_on='Vp-Code')#\n",
    "#export the edited DF which include all data from all tasks and questionnaires\n",
    "\n",
    "Dom_final.drop(columns=['Progress_y',\"Duration (in seconds)\",'Demo_employment_7_TEXT'],inplace=True)\n",
    "df_concat=pd.ExcelWriter('opisi.xlsx')\n",
    "Dom_final.to_excel(df_concat)\n",
    "df_concat.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
